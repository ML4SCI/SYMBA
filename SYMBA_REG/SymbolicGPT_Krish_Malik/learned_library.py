# -*- coding: utf-8 -*-
"""Learned_library.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YE9Q-LpODpkLvZckwQ9uVJuihbIuEBD0
"""

import json
from collections import Counter
from pathlib import Path

# Load the uploaded JSON file
file_path = Path("/content/tokenized_gpt_labels_edit.json")
with file_path.open("r") as f:
    data = json.load(f)

# Reverse vocabulary: id -> token
vocab = data["vocab"]
id_to_token = {v: k for k, v in vocab.items()}
tokenized_trees = data["tokenized_trees"]

# Subtree extractor with extended max length
def extract_subtrees(tokens, max_len=12):
    subtrees = []

    def helper(idx):
        token = tokens[idx]
        if token in {"add", "sub", "mul", "div", "pow"}:
            l_size, _ = helper(idx + 1)
            r_size, _ = helper(idx + 1 + l_size)
            total_size = 1 + l_size + r_size
        elif token in {"sin", "cos", "log", "exp", "tanh"}:
            arg_size, _ = helper(idx + 1)
            total_size = 1 + arg_size
        else:
            total_size = 1

        # Only add if subtree fits length constraints
        if 4 <= total_size <= max_len:
            subtree = tuple(tokens[idx:idx + total_size])
            subtrees.append(subtree)

        return total_size, None

    helper(0)
    return subtrees

# Mine subtrees from all tokenized trees
subtree_counter = Counter()

for id_tree in tokenized_trees:
    # Remove EOS token if present
    if id_tree and id_tree[-1] == vocab.get("EOS"):
        id_tree = id_tree[:-1]

    # Convert token ids to tokens
    tokens = [id_to_token[id] for id in id_tree]

    # Extract and count subtrees
    subtrees = extract_subtrees(tokens)
    subtree_counter.update(subtrees)

# Print the most common subtrees
print("Most common subtrees:")
for subtree, count in subtree_counter.most_common(20):
    print(f"{count}: {' '.join(subtree)}")