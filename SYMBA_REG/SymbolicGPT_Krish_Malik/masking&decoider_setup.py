# -*- coding: utf-8 -*-
"""Masking&Decoider_Setup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12aTNwyQ8yOakz74g8RN_Pr6Z3qH4axhZ
"""

# Step 1: Upload your JSON file
from google.colab import files
uploaded = files.upload()

#Step 2: Load and process the JSON file
import json
import re

# Load uploaded file
filename = list(uploaded.keys())[0]
with open(filename, 'r') as f:
    data = json.load(f)

#Step 3: Replace all constants with ⟨C⟩
for entry in data:
    tree = entry["symbolic_parse_tree"]
    # Replace all const(...) patterns
    masked_tree = re.sub(r'const\(([^)]+)\)', 'const(⟨C⟩)', tree)
    entry["masked_parse_tree"] = masked_tree  # Add a new key with the masked version

#Step 4: Save output JSON
masked_filename = "masked_parse_trees.json"
with open(masked_filename, 'w') as f:
    json.dump(data, f, indent=2)
#Step 5: Download the output
files.download("masked_parse_trees.json")

#\u27e8 is the Unicode for ⟨ (LEFT ANGLE BRACKET)
#C is literal
#\u27e9 is the Unicode for ⟩ (RIGHT ANGLE BRACKET), JSON uses Unicode escape sequences for non-ASCII characters

import json
from google.colab import files

#Step 1: Upload your masked tree JSON file
uploaded = files.upload()  # Upload `masked_parse_trees.json`
filename = list(uploaded.keys())[0]

#Step 2: Load the uploaded JSON
with open(filename, 'r') as f:
    data = json.load(f)

#Step 3: Convert stringified masked parse trees into nested lists
def str_expr_to_tree(expr_str):
    """
    Parses a function-style string like:
    'pow(add(id(x), ⟨C⟩), ⟨C⟩)'
    into a proper nested list:
    ['pow', ['add', ['id', 'x'], '⟨C⟩'], '⟨C⟩']
    """

    def tokenize(s):
        tokens = []
        current = ''
        for c in s:
            if c in '(),':
                if current:
                    tokens.append(current)
                    current = ''
                tokens.append(c)
            elif c.isspace():
                if current:
                    tokens.append(current)
                    current = ''
            else:
                current += c
        if current:
            tokens.append(current)
        return tokens

    def parse(tokens):
        def parse_expr():
            if not tokens:
                return None

            token = tokens.pop(0)

            # Function call: token followed by '('
            if token not in ('(', ')', ','):
                if tokens and tokens[0] == '(':
                    func_name = token
                    tokens.pop(0)  # remove '('
                    args = []
                    while tokens and tokens[0] != ')':
                        args.append(parse_expr())
                        if tokens and tokens[0] == ',':
                            tokens.pop(0)  # remove comma
                    if tokens and tokens[0] == ')':
                        tokens.pop(0)  # remove ')'
                    return [func_name] + args
                else:
                    # Just a single token, e.g. 'x', '⟨C⟩'
                    return token

            elif token == '(':
                # Parentheses group without function name
                expr = parse_expr()
                if tokens and tokens[0] == ')':
                    tokens.pop(0)
                return expr

            # If token is ')' or ',' at this point, return None or handle error
            return None

        return parse_expr()

    tokens = tokenize(expr_str)
    parsed_tree = parse(tokens)
    return parsed_tree

#Step 4: Flatten nested symbolic trees to prefix token sequences
def flatten_tree(tree):
    if isinstance(tree, str):
        return [tree]
    elif isinstance(tree, list):
        return [tree[0]] + [t for arg in tree[1:] for t in flatten_tree(arg)]
    else:
        raise ValueError("Unexpected tree format")

#Step 5: Build clean tokenized trees (prefix format)
tokenized_trees = []
for entry in data:
    expr = entry['masked_parse_tree']
    tree = str_expr_to_tree(expr)
    prefix_tokens = flatten_tree(tree)
    tokenized_trees.append(prefix_tokens)

#Step 6: Build vocab
vocab = {}
for tokens in tokenized_trees:
    for tok in tokens:
        if tok not in vocab:
            vocab[tok] = len(vocab)

#Step 7: Add special tokens
vocab["<PAD>"] = len(vocab)
vocab["<EOS>"] = len(vocab)

#Step 8: Convert tokenized trees to ID sequences (with <EOS>)
tokenized_id_seqs = []
for tokens in tokenized_trees:
    ids = [vocab[tok] for tok in tokens] + [vocab["<EOS>"]]
    tokenized_id_seqs.append(ids)

#Step 9: Save to JSON file
output = {
    "vocab": vocab,
    "tokenized_trees": tokenized_id_seqs
}

with open("tokenized_gpt_labels_edit.json", 'w') as f:
    json.dump(output, f, indent=2)

#Step 10: Download the result
files.download("tokenized_gpt_labels_edit.json")