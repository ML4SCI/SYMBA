# -*- coding: utf-8 -*-
"""Decoder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jeXv15WeSVevipWbeEQ__VirT1fN6q8z
"""

# 1. Load and Prepare Data
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

with open("tnet_embeddings.json", "r") as f:
    embeddings_data = json.load(f)
with open("tokenized_gpt_labels_edit.json", "r") as f:
    label_data = json.load(f)

X = [torch.tensor(e["embedding"], dtype=torch.float32) for e in embeddings_data]
Y = label_data["tokenized_trees"]
vocab = label_data["vocab"]

# Add special tokens
if "<PAD>" not in vocab:
    vocab["<PAD>"] = max(vocab.values()) + 1
if "<EOS>" not in vocab:
    vocab["<EOS>"] = max(vocab.values()) + 1
if "<BOS>" not in vocab:
    vocab["<BOS>"] = max(vocab.values()) + 1

pad_token_id = vocab["<PAD>"]
eos_token_id = vocab["<EOS>"]
bos_token_id = vocab["<BOS>"]

# Append EOS to each sequence
Y = [seq + [eos_token_id] for seq in Y]

# 2. Dataset & DataLoader
class SymbolicDataset(Dataset):
    def __init__(self, embeddings, token_seqs, pad_token_id, bos_token_id, eos_token_id, max_len=None):
        self.embeddings = embeddings
        self.pad_token_id = pad_token_id
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.max_len = max_len or max(len(seq) + 1 for seq in token_seqs)  # +1 for BOS

        self.decoder_inputs = []
        self.targets = []

        for seq in token_seqs:
            decoder_input = [bos_token_id] + seq  # prepend BOS
            target = seq + [eos_token_id]

            # Pad both to max_len
            padding_len = self.max_len - len(decoder_input)
            decoder_input += [pad_token_id] * padding_len
            target += [pad_token_id] * padding_len

            self.decoder_inputs.append(decoder_input)
            self.targets.append(target)

    def __len__(self):
        return len(self.embeddings)

    def __getitem__(self, idx):
        x = self.embeddings[idx]
        dec_in = torch.tensor(self.decoder_inputs[idx], dtype=torch.long)
        tgt = torch.tensor(self.targets[idx], dtype=torch.long)
        attention_mask = (dec_in != self.pad_token_id)
        return {
            "embedding": x,
            "decoder_input": dec_in,
            "target": tgt,
            "attention_mask": attention_mask
        }

dataset = SymbolicDataset(X, Y, pad_token_id, bos_token_id, eos_token_id)
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

# 3.Decoder Model
class SymbolicDecoder(nn.Module):
    def __init__(self, vocab_size, embedding_dim=128, gpt_dim=256, n_layers=4, n_heads=4, max_len=100):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, gpt_dim)
        self.pos_embedding = nn.Parameter(torch.randn(1, max_len + 1, gpt_dim))
        self.embedding_proj = nn.Linear(embedding_dim, gpt_dim)
        self.dropout = nn.Dropout(0.2)
        self.norm = nn.LayerNorm(gpt_dim)

        decoder_layer = nn.TransformerDecoderLayer(
            d_model=gpt_dim,
            nhead=n_heads,
            batch_first=True,
            dropout=0.2
        )
        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)
        self.out = nn.Linear(gpt_dim, vocab_size)

    def forward(self, embedding, decoder_input_ids):
        B, T = decoder_input_ids.shape
        tok_embed = self.token_embedding(decoder_input_ids)  # [B, T, D]
        context_tok = self.embedding_proj(embedding).unsqueeze(1)  # [B, 1, D]
        x = torch.cat([context_tok, tok_embed], dim=1)  # [B, T+1, D]
        x = x + self.pos_embedding[:, :T+1, :]
        x = self.dropout(self.norm(x))

        # Causal mask
        causal_mask = torch.tril(torch.ones(T+1, T+1, device=decoder_input_ids.device)).bool()
        causal_mask = ~causal_mask

        out = self.transformer(
            tgt=x,
            memory=context_tok,
            tgt_mask=causal_mask
        )
        logits = self.out(out[:, 1:, :])  # drop context token output
        return logits

#4. Training Loop
import torch.optim as optim

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SymbolicDecoder(
    vocab_size=len(vocab),
    embedding_dim=128, # remains constant as data is in this form
    gpt_dim=512,
    n_layers=6,
    n_heads=8,
    max_len=dataset.max_len
).to(device)

criterion = nn.CrossEntropyLoss(ignore_index=-100)
optimizer = optim.AdamW(model.parameters(), lr=1e-4)

num_epochs = 50  # Can be lowered to 10 for quick runs
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for batch in dataloader:
        embedding = batch["embedding"].to(device)
        decoder_input = batch["decoder_input"].to(device)
        target = batch["target"].to(device)

        labels = torch.where(target == pad_token_id, -100, target)
        logits = model(embedding, decoder_input)
        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1} | Avg Loss: {total_loss / len(dataloader):.4f}")

# 5. Save Model
torch.save(model.state_dict(), "symbolic_gpt_decoder_bos.pth")

# 6. Updated Inference (Greedy Decode)
def greedy_decode(model, embedding, max_len=100):
    model.eval()
    with torch.no_grad():
        input_ids = torch.tensor([[bos_token_id]], dtype=torch.long).to(device)
        for _ in range(max_len):
            logits = model(embedding.unsqueeze(0).to(device), input_ids)
            next_token = logits[0, -1].argmax().item()
            input_ids = torch.cat([input_ids, torch.tensor([[next_token]], device=device)], dim=1)
            if next_token == eos_token_id:
                break
        return input_ids.squeeze().tolist()[1:]

def decode_embedding_to_tokens(embedding_tensor):
    token_ids = greedy_decode(model, embedding_tensor)
    id_to_token = {v: k for k, v in vocab.items()}
    return [id_to_token.get(i, f"<UNK_{i}>") for i in token_ids]