{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed728a92",
   "metadata": {},
   "source": [
    "### This Jupyter notebook is an implementation of Vanilla/Traditional Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c4cbc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52d8a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Setting up Transformers Hyperparameters/Configuration, \n",
    "These Parameters are tunable\n",
    "\"\"\"\n",
    "class HyperParams:\n",
    "    def __init__(self):\n",
    "        self.d_x = 512\n",
    "        self.n_layers = 6\n",
    "        self.n_heads = 8\n",
    "        self.d_k = self.d_x // self.n_heads\n",
    "        self.d_v = self.d_x // self.n_heads\n",
    "        self.dropout = 0.1\n",
    "        self.max_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f864d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tokenizer mechanism for Transformers + building vocabulary for\n",
    "QED 2-2 particle interaction Dataset\n",
    "\"\"\"\n",
    "class QEDTokenizer:\n",
    "    def __init__(self):\n",
    "        self.special_tokens = {\"<PAD>\": 0, \"<UNK>\": 1, \"<BOS>\": 2, \"<EOS>\": 3}\n",
    "        self.vocab = self.special_tokens.copy()\n",
    "        self.next_id = len(self.vocab)\n",
    "        self.operators = [\"+\", \"-\", \"*\", \"/\", \"^\"]\n",
    "        self.variables = [\"m_d\", \"m_u\", \"s_11\", \"s_12\"]\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.vocab:\n",
    "            self.vocab[token] = self.next_id\n",
    "            self.next_id += 1\n",
    "\n",
    "    def build_vocab(self, expressions):\n",
    "        for expr in expressions:\n",
    "            tokens = self.tokenize(expr)\n",
    "            for token in tokens:\n",
    "                self.add_token(token)\n",
    "\n",
    "    def tokenize(self, expr):\n",
    "        tokens = []\n",
    "        expr = expr.replace(\" \", \"\")\n",
    "        i = 0\n",
    "        while i < len(expr):\n",
    "            matched = False\n",
    "            for var in self.variables:\n",
    "                if expr[i:].startswith(var):\n",
    "                    tokens.append(var)\n",
    "                    i += len(var)\n",
    "                    matched = True\n",
    "                    break\n",
    "            if matched:\n",
    "                continue\n",
    "            if expr[i] in self.operators:\n",
    "                tokens.append(expr[i])\n",
    "                i += 1\n",
    "                continue\n",
    "            num = \"\"\n",
    "            while i < len(expr) and (expr[i].isdigit() or expr[i] == \".\"):\n",
    "                num += expr[i]\n",
    "                i += 1\n",
    "            if num:\n",
    "                tokens.append(num)\n",
    "                continue\n",
    "            i += 1\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, expr, max_length):\n",
    "        tokens = self.tokenize(expr)\n",
    "        ids = [self.vocab[\"<BOS>\"]] + [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in tokens] + [self.vocab[\"<EOS>\"]]\n",
    "        ids = ids[:max_length]\n",
    "        ids += [self.vocab[\"<PAD>\"]] * (max_length - len(ids))\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        tokens = [k for id in ids for k, v in self.vocab.items() if v == id and k not in [\"<PAD>\", \"<BOS>\", \"<EOS>\"]]\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "893e5fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QEDDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = str(self.data.iloc[idx][\"amp\"])\n",
    "        trg = str(self.data.iloc[idx][\"sqamp\"])\n",
    "        src_ids = self.tokenizer.encode(src, self.max_length)\n",
    "        trg_ids = self.tokenizer.encode(trg, self.max_length)\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(src_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(trg_ids, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e94afba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining standard Embedding mechanism with positional encoding, \n",
    "Reference : https://arxiv.org/abs/1706.03762\n",
    "\"\"\"\n",
    "class StandardEmbedding(nn.Module):\n",
    "    def __init__(self, d_vocab, d_x, dropout, max_length):\n",
    "        super().__init__()\n",
    "        self.d_x = d_x\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.tok_embedding = nn.Embedding(d_vocab, d_x)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_x]))\n",
    "        pe = torch.zeros(max_length, d_x)\n",
    "        position = torch.arange(0., max_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_x, 2) * -(math.log(10000.0) / d_x))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if src.max().item() >= self.tok_embedding.num_embeddings:\n",
    "            raise ValueError(f\"Input indices {src.max().item()} exceed vocab size {self.tok_embedding.num_embeddings}\")\n",
    "        tok_emb = self.tok_embedding(src) * self.scale.to(src.device)\n",
    "        pos_emb = self.pe[:, :src.size(1)]\n",
    "        x = tok_emb + pos_emb\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec07c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi Head Attention Module, \n",
    "with QKV vectors, dim, d_heads, and dropout can be tuned\n",
    "\"\"\"\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_x, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.d_x = d_x\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_x // n_heads\n",
    "        self.W_q = nn.Linear(d_x, d_x)\n",
    "        self.W_k = nn.Linear(d_x, d_x)\n",
    "        self.W_v = nn.Linear(d_x, d_x)\n",
    "        self.W_o = nn.Linear(d_x, d_x)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        bsz = query.size(0)\n",
    "        Q = self.W_q(query).view(bsz, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(bsz, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(bsz, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn = self.dropout(F.softmax(scores, dim=-1))\n",
    "        context = torch.matmul(attn, V)\n",
    "        context = context.transpose(1, 2).contiguous().view(bsz, -1, self.d_x)\n",
    "        return self.W_o(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9121f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Standard Encoder Layer Definition, With MHA as attention mechanism \n",
    "Dropout and heads are passed as hyperparams instance\n",
    "\"\"\"\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(p.d_x, p.n_heads, p.dropout)\n",
    "        self.ff = nn.Linear(p.d_x, p.d_x)\n",
    "        self.norm1 = nn.LayerNorm(p.d_x)\n",
    "        self.norm2 = nn.LayerNorm(p.d_x)\n",
    "        self.dropout = nn.Dropout(p.dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        z = self.norm1(src)\n",
    "        z = self.attn(z, z, z, src_mask)\n",
    "        src = src + self.dropout(z)\n",
    "        z = self.norm2(src)\n",
    "        z = F.relu(self.ff(z))\n",
    "        src = src + self.dropout(z)\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f102a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encoder Block for Transformer\n",
    "\"\"\"\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(p) for _ in range(p.n_layers)])\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa5b9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decoder Layer Definition for Transformer\n",
    "\"\"\"\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(p.d_x, p.n_heads, p.dropout)\n",
    "        self.enc_attn = MultiHeadAttention(p.d_x, p.n_heads, p.dropout)\n",
    "        self.ff = nn.Linear(p.d_x, p.d_x)\n",
    "        self.norm1 = nn.LayerNorm(p.d_x)\n",
    "        self.norm2 = nn.LayerNorm(p.d_x)\n",
    "        self.norm3 = nn.LayerNorm(p.d_x)\n",
    "        self.dropout = nn.Dropout(p.dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        z = self.norm1(trg)\n",
    "        z = self.self_attn(z, z, z, trg_mask)\n",
    "        trg = trg + self.dropout(z)\n",
    "        z = self.norm2(trg)\n",
    "        z = self.enc_attn(z, enc_src, enc_src, src_mask)\n",
    "        trg = trg + self.dropout(z)\n",
    "        z = self.norm3(trg)\n",
    "        z = F.relu(self.ff(z))\n",
    "        trg = trg + self.dropout(z)\n",
    "        return trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e70e5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decoder Block Implementation\n",
    "\"\"\"\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(p) for _ in range(p.n_layers)])\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        return trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a1ee3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformer Module Definition, with module hyperparams passed as \n",
    "arguments, \n",
    "\n",
    "d_vocab = vocabulary dimension\n",
    "n_layers = number of layers \n",
    "n_heads = number of heads\n",
    "dropout = dropout value \n",
    "max_length = maximum length for vocab \n",
    "pad_idx = index padding for compatibility\n",
    "\"\"\"\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_vocab, d_x, n_layers, n_heads, dropout, max_length, pad_idx):\n",
    "        super().__init__()\n",
    "        self.p = HyperParams()\n",
    "        self.p.d_x = d_x\n",
    "        self.p.n_layers = n_layers\n",
    "        self.p.n_heads = n_heads\n",
    "        self.p.dropout = dropout\n",
    "        self.p.max_length = max_length\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embedding = StandardEmbedding(d_vocab, d_x, dropout, max_length)\n",
    "        self.encoder = Encoder(self.p)\n",
    "        self.decoder = Decoder(self.p)\n",
    "        self.out = nn.Linear(d_x, d_vocab)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def make_masks(self, src, trg):\n",
    "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_pad_mask = (trg != self.pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len, device=trg.device)).bool()\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return src_mask, trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask, trg_mask = self.make_masks(src, trg)\n",
    "        src_emb = self.embedding(src)\n",
    "        trg_emb = self.embedding(trg)\n",
    "        enc_src = self.encoder(src_emb, src_mask)\n",
    "        dec_out = self.decoder(trg_emb, enc_src, trg_mask, src_mask)\n",
    "        logits = self.out(dec_out)\n",
    "        return logits\n",
    "\n",
    "    def greedy_inference(self, src, sos_idx, eos_idx, max_length):\n",
    "        self.eval()\n",
    "        src = src.to(self.device)\n",
    "        batch_size = src.size(0)\n",
    "        src_mask = self.make_masks(src, src)[0]\n",
    "        enc_src = self.encoder(self.embedding(src), src_mask)\n",
    "        trg = torch.full((batch_size, 1), sos_idx, dtype=torch.long, device=self.device)\n",
    "        for _ in range(max_length):\n",
    "            trg_mask = self.make_masks(trg, trg)[1]\n",
    "            out = self.decoder(self.embedding(trg), enc_src, trg_mask, src_mask)\n",
    "            logits = self.out(out[:, -1])\n",
    "            pred = torch.argmax(logits, dim=-1).unsqueeze(1)\n",
    "            trg = torch.cat([trg, pred], dim=1)\n",
    "            if torch.all(pred == eos_idx):\n",
    "                break\n",
    "        return trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "974ec622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train and Evaluate function declaration\n",
    "\"\"\"\n",
    "def train_and_evaluate(model, train_loader, val_loader, epochs, lr, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=model.pad_idx)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        for batch in train_loader:\n",
    "            src = batch[\"input_ids\"].to(device)\n",
    "            trg = batch[\"labels\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, trg[:, :-1])\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            target = trg[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(output, dim=-1)\n",
    "            mask = target != model.pad_idx\n",
    "            train_correct += (preds[mask] == target[mask]).sum().item()\n",
    "            train_total += mask.sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / train_total if train_total > 0 else 0\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                src = batch[\"input_ids\"].to(device)\n",
    "                trg = batch[\"labels\"].to(device)\n",
    "                output = model(src, trg[:, :-1])\n",
    "                output = output.view(-1, output.size(-1))\n",
    "                target = trg[:, 1:].contiguous().view(-1)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.argmax(output, dim=-1)\n",
    "                mask = target != model.pad_idx\n",
    "                val_correct += (preds[mask] == target[mask]).sum().item()\n",
    "                val_total += mask.sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total if val_total > 0 else 0\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}: \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2%}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "189da50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Loss: 21.3523, Train Acc: 25.40%, Val Loss: 8.6312, Val Acc: 46.46%\n",
      "Epoch 2/10: Train Loss: 9.5208, Train Acc: 45.39%, Val Loss: 5.8563, Val Acc: 54.04%\n",
      "Epoch 3/10: Train Loss: 7.8163, Train Acc: 47.35%, Val Loss: 5.4406, Val Acc: 47.78%\n",
      "Epoch 4/10: Train Loss: 7.0167, Train Acc: 47.31%, Val Loss: 5.5056, Val Acc: 56.95%\n",
      "Epoch 5/10: Train Loss: 6.5725, Train Acc: 49.14%, Val Loss: 3.4354, Val Acc: 59.47%\n",
      "Epoch 6/10: Train Loss: 6.0060, Train Acc: 49.90%, Val Loss: 4.2898, Val Acc: 57.00%\n",
      "Epoch 7/10: Train Loss: 5.9740, Train Acc: 49.31%, Val Loss: 4.1054, Val Acc: 59.75%\n",
      "Epoch 8/10: Train Loss: 5.7324, Train Acc: 50.38%, Val Loss: 3.4809, Val Acc: 57.11%\n",
      "Epoch 9/10: Train Loss: 5.5991, Train Acc: 51.41%, Val Loss: 3.4617, Val Acc: 59.42%\n",
      "Epoch 10/10: Train Loss: 5.4618, Train Acc: 51.85%, Val Loss: 3.2606, Val Acc: 59.91%\n",
      "Input: 1/9*i*e^2*(p_2_\\INDEX_0*gamma_{+\\INDEX_0,INDEX_1,INDEX_2}*gamma_{\\INDEX_3,INDEX_4,INDEX_1}*gamma_{\\INDEX_5,INDEX_2,INDEX_6}*A_{MOMENTUM_0,+\\INDEX_5}(p_3)^(*)*A_{MOMENTUM_1,+\\INDEX_3}(p_4)^(*)*b_{MOMENTUM_2,INDEX_6}(p_2)_u*b_{MOMENTUM_3,INDEX_4}(p_1)_v^(*)+-p_3_\\INDEX_0*gamma_{+\\INDEX_0,INDEX_7,INDEX_8}*gamma_{\\INDEX_3,INDEX_9,INDEX_7}*gamma_{\\INDEX_5,INDEX_8,INDEX_10}*A_{MOMENTUM_0,+\\INDEX_5}(p_3)^(*)*A_{MOMENTUM_1,+\\INDEX_3}(p_4)^(*)*b_{MOMENTUM_2,INDEX_10}(p_2)_u*b_{MOMENTUM_3,INDEX_9}(p_1)_v^(*)+m_b*gamma_{\\INDEX_3,INDEX_11,INDEX_12}*gamma_{\\INDEX_5,INDEX_12,INDEX_13}*A_{MOMENTUM_0,+\\INDEX_5}(p_3)^(*)*A_{MOMENTUM_1,+\\INDEX_3}(p_4)^(*)*b_{MOMENTUM_2,INDEX_13}(p_2)_u*b_{MOMENTUM_3,INDEX_11}(p_1)_v^(*))/(m_b^2+-s_22+2*s_23+-s_33+-reg_prop)\n",
      "Output IDs: [[2, 16, 13, 7, 8, 13, 7, 8, 13, 7, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 22, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5, 8, 9, 7, 17, 10, 16, 4, 5]]\n",
      "Output: -4*^4*^4*^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-22+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/^2*23+-1/\n"
     ]
    }
   ],
   "source": [
    "d_x = 512\n",
    "n_layers = 6\n",
    "n_heads = 8\n",
    "dropout = 0.1\n",
    "max_length = 300\n",
    "pad_idx = 0\n",
    "sos_idx = 2\n",
    "eos_idx = 3\n",
    "epochs = 10\n",
    "lr = 1e-4\n",
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_df = pd.read_csv(r'D:\\DecoderKAN\\QED_data\\test-flow.csv') # test-flow, is considerably smaller dataset, to train quickly and check the model performance\n",
    "\n",
    "\n",
    "tokenizer = QEDTokenizer()\n",
    "expressions = pd.concat([data_df[\"amp\"], data_df[\"sqamp\"]]).tolist()\n",
    "tokenizer.build_vocab(expressions)\n",
    "d_vocab = len(tokenizer.get_vocab())\n",
    "\n",
    "# Dataset\n",
    "dataset = QEDDataset(data_df, tokenizer, max_length)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = Subset(dataset, range(train_size)), Subset(dataset, range(train_size, len(dataset)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Model\n",
    "model = Transformer(d_vocab, d_x, n_layers, n_heads, dropout, max_length, pad_idx)\n",
    "model.to(device)\n",
    "\n",
    "# Train\n",
    "train_and_evaluate(model, train_loader, val_loader, epochs, lr, device)\n",
    "\n",
    "# Test Inference\n",
    "model.eval()\n",
    "test_expr = r\"1/9*i*e^2*(p_2_\\INDEX_0*gamma_{+\\INDEX_0,INDEX_1,INDEX_2}*gamma_{\\INDEX_3,INDEX_4,INDEX_1}*gamma_{\\INDEX_5,INDEX_2,INDEX_6}*A_{MOMENTUM_0,+\\INDEX_5}(p_3)^(*)*A_{MOMENTUM_1,+\\INDEX_3}(p_4)^(*)*b_{MOMENTUM_2,INDEX_6}(p_2)_u*b_{MOMENTUM_3,INDEX_4}(p_1)_v^(*)+-p_3_\\INDEX_0*gamma_{+\\INDEX_0,INDEX_7,INDEX_8}*gamma_{\\INDEX_3,INDEX_9,INDEX_7}*gamma_{\\INDEX_5,INDEX_8,INDEX_10}*A_{MOMENTUM_0,+\\INDEX_5}(p_3)^(*)*A_{MOMENTUM_1,+\\INDEX_3}(p_4)^(*)*b_{MOMENTUM_2,INDEX_10}(p_2)_u*b_{MOMENTUM_3,INDEX_9}(p_1)_v^(*)+m_b*gamma_{\\INDEX_3,INDEX_11,INDEX_12}*gamma_{\\INDEX_5,INDEX_12,INDEX_13}*A_{MOMENTUM_0,+\\INDEX_5}(p_3)^(*)*A_{MOMENTUM_1,+\\INDEX_3}(p_4)^(*)*b_{MOMENTUM_2,INDEX_13}(p_2)_u*b_{MOMENTUM_3,INDEX_11}(p_1)_v^(*))/(m_b^2+-s_22+2*s_23+-s_33+-reg_prop)\"\n",
    "src_ids = torch.tensor([tokenizer.encode(test_expr, max_length)], device=device)\n",
    "output = model.greedy_inference(src_ids, sos_idx, eos_idx, max_length)\n",
    "decoded = tokenizer.decode(output[0].tolist())\n",
    "print(f\"Input: {test_expr}\")\n",
    "print(f\"Output IDs: {output.tolist()}\")\n",
    "print(f\"Output: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eea7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
