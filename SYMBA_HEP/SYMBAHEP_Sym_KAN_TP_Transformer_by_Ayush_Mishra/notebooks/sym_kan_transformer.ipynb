{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c06437d",
   "metadata": {},
   "source": [
    "This is the Implementation of Symbolic KAN Transformer, this module uses Role Filler Embeddings, Gating Mechanism, Sinusoidal Representational Networks, and SineKAN mechanism,  The model tend to perform better than vanilla transformers, and other Rational KAN and Fourier KAN, with a reasonable time taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf9325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from typing import List, Union, OrderedDict, Tuple\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f69ec587",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of Role Filler Embeddings, Role Filler Embeddings consits \n",
    "of Two components, role and a filler component, \n",
    "where r  = linear(x) + 1 and , z = x * r\n",
    "\"\"\"\n",
    "\n",
    "class RoleFillerEmbedding(nn.Module):\n",
    "    def __init__(self, d_vocab, d_x, dropout, max_length):\n",
    "        super().__init__()\n",
    "        self.d_x = d_x\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.tok_embedding = nn.Embedding(d_vocab, d_x)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([d_x]))\n",
    "        pe = torch.zeros(max_length, d_x)\n",
    "        position = torch.arange(0., max_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0., d_x, 2) * -(math.log(10000.0) / d_x))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.linear = nn.Linear(d_x, d_x)\n",
    "        nn.init.normal_(self.linear.weight, mean=0, std=1./math.sqrt(d_x))\n",
    "        nn.init.zeros_(self.linear.bias)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if src.max().item() >= self.tok_embedding.num_embeddings:\n",
    "            raise ValueError(f\"Input indices {src.max().item()} exceed vocab size {self.tok_embedding.num_embeddings}\")\n",
    "        tok_emb = self.tok_embedding(src) * self.scale.to(src.device)\n",
    "        seq_length = src.size(1)\n",
    "        pos_emb = self.pe[:, :seq_length, :]  # Slice to match input length\n",
    "        x = tok_emb + pos_emb\n",
    "        r = self.linear(x) + 1\n",
    "        z = x * r\n",
    "        return self.dropout(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7414ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This Module contains the implementation of SineKAN Mechanism, with Gating Mechansim \n",
    "The Gating Mechanism is applied after performing Sinusoidal Operation, The Gate is taken\n",
    "as a Learnable Parameter\n",
    "SineKAN Paper : https://arxiv.org/abs/2407.04149\n",
    "SineKAN Implementation: https://github.com/ereinha/SineKAN\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def forward_step(i_n, grid_size, A, K, C):\n",
    "    ratio = A * grid_size**(-K) + C\n",
    "    i_n1 = ratio * i_n\n",
    "    return i_n1\n",
    "\n",
    "class SineKANLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device='cuda', grid_size=8, is_first=False, add_bias=True, norm_freq=True):\n",
    "        super(SineKANLayer, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.device = device\n",
    "        self.is_first = is_first\n",
    "        self.add_bias = add_bias\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.A, self.K, self.C = 0.9724108095811765, 0.9884401790754128, 0.999449553483052\n",
    "        \n",
    "        self.grid_norm_factor = (torch.arange(grid_size) + 1)\n",
    "        self.grid_norm_factor = self.grid_norm_factor.reshape(1, 1, grid_size)\n",
    "            \n",
    "        if is_first:\n",
    "            self.amplitudes = torch.nn.Parameter(torch.empty(output_dim, input_dim, 1).normal_(0, .4) / output_dim / self.grid_norm_factor)\n",
    "        else:\n",
    "            self.amplitudes = torch.nn.Parameter(torch.empty(output_dim, input_dim, 1).uniform_(-1, 1) / output_dim / self.grid_norm_factor)\n",
    "\n",
    "        grid_phase = torch.arange(1, grid_size + 1).reshape(1, 1, 1, grid_size) / (grid_size + 1)\n",
    "        self.input_phase = torch.linspace(0, math.pi, input_dim).reshape(1, 1, input_dim, 1).to(device)\n",
    "        phase = grid_phase.to(device) + self.input_phase\n",
    "\n",
    "        if norm_freq:\n",
    "            self.freq = torch.nn.Parameter(torch.arange(1, grid_size + 1).float().reshape(1, 1, 1, grid_size) / (grid_size + 1)**(1 - is_first))\n",
    "        else:\n",
    "            self.freq = torch.nn.Parameter(torch.arange(1, grid_size + 1).float().reshape(1, 1, 1, grid_size))\n",
    "\n",
    "        for i in range(1, self.grid_size):\n",
    "            phase = forward_step(phase, i, self.A, self.K, self.C)\n",
    "        self.register_buffer('phase', phase)\n",
    "        \n",
    "        # Dynamic gate computation using a linear layer\n",
    "        self.gate_linear = nn.Linear(input_dim, input_dim)  # Gate operates on input dimension\n",
    "        nn.init.xavier_uniform_(self.gate_linear.weight)\n",
    "        nn.init.zeros_(self.gate_linear.bias)\n",
    "        \n",
    "        if self.add_bias:\n",
    "            self.bias = torch.nn.Parameter(torch.ones(1, output_dim) / output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = x.shape\n",
    "        output_shape = x_shape[0:-1] + (self.output_dim,)\n",
    "        batch_size = x.shape[0]\n",
    "        x = torch.reshape(x, (-1, self.input_dim))\n",
    "        x_reshaped = x.unsqueeze(1).unsqueeze(-1)  # [batch_size, 1, input_dim, 1]\n",
    "        \n",
    "        # Validate shapes\n",
    "        assert x_reshaped.size(2) == self.input_dim, f\"Input dimension mismatch: expected {self.input_dim}, got {x_reshaped.size(2)}\"\n",
    "        assert self.freq.size(3) == self.grid_size, f\"Frequency grid size mismatch: expected {self.grid_size}, got {self.freq.size(3)}\"\n",
    "        assert self.phase.size(2) == self.input_dim and self.phase.size(3) == self.grid_size, f\"Phase shape mismatch: expected ({self.input_dim}, {self.grid_size}), got {self.phase.shape[2:]}\"\n",
    "        \n",
    "        # Compute sine term\n",
    "        s = torch.sin(x_reshaped * self.freq + self.phase)  # [batch_size, 1, input_dim, grid_size]\n",
    "        \n",
    "        # Compute dynamic gate\n",
    "        gate = torch.sigmoid(self.gate_linear(x))  # [batch_size, input_dim]\n",
    "        gate = gate.unsqueeze(1).unsqueeze(-1)  # [batch_size, 1, input_dim, 1]\n",
    "        \n",
    "        # Apply gating\n",
    "        gated_s = s * gate  # [batch_size, 1, input_dim, grid_size]\n",
    "        \n",
    "        # Einsum with amplitudes\n",
    "        y = torch.einsum('ijkl,jkl->ij', gated_s, self.amplitudes)  # [batch_size, output_dim]\n",
    "        if self.add_bias:\n",
    "            y += self.bias\n",
    "        y = torch.reshape(y, output_shape)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b1aa651",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FeedForward Block, ff_dims, in_size and grid_size parameters are tunable, \n",
    "uses SineKAN block as the layer, this is used \n",
    "as the output layer in the implementation\n",
    "\"\"\"\n",
    "class KANFeedForwardBlock(nn.Module):\n",
    "    def __init__(self, in_size: int, ff_dims: List[int], grid_size: int = 8, device: Union[str, int] = 'cuda') -> None:\n",
    "        super().__init__()\n",
    "        self.ffn = nn.ModuleList()\n",
    "        for i, d in enumerate(ff_dims):\n",
    "            self.ffn.append(SineKANLayer(\n",
    "                in_size, d, grid_size=grid_size, device=device, is_first=(i == 0)))\n",
    "            in_size = d\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for f in self.ffn:\n",
    "            x = f(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6d69a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Layer Normalization Block\n",
    "\"\"\"\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features: int, eps: float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7307cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44cff24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Positional Encoding Implementation for Embeddings\"\"\"\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e65f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, features: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "051d820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Standard Multi Head Attention Mechanism Implementation\n",
    "\"\"\"\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff403573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encoder Block and Encoder Layer Implementation\n",
    "\"\"\"\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, ff_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.ff_block = ff_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.ff_block)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a8a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decoder Block and Decoder Layer Implementation\n",
    "\"\"\"\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, \n",
    "                 feed_forward_block: Union[KANFeedForwardBlock, FeedForwardBlock], dropout: float, is_kan: bool) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.ff_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)]) if is_kan else nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "        self.is_kan = is_kan\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        if self.is_kan:\n",
    "            x = self.ff_block(x)\n",
    "        else:\n",
    "            x = self.residual_connections[2](x, self.ff_block)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a06b554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d265f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transformer Implementation, Hyperparams are tunable, check the below code\n",
    "\"\"\"\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: RoleFillerEmbedding, tgt_embed: RoleFillerEmbedding, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3be8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kanformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, \n",
    "                    N: int=3, h: int=8, dropout: float=0.1, d_ff: int=4096, ff_dims: List[int]=[8192], device: Union[str, int] = 'cuda') -> Transformer:\n",
    "    src_embed = RoleFillerEmbedding(src_vocab_size, d_model, dropout, src_seq_len)\n",
    "    tgt_embed = RoleFillerEmbedding(tgt_vocab_size, d_model, dropout, tgt_seq_len)\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "    \n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        ff_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, ff_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    decoder_blocks = []\n",
    "    for i in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        ff_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        kan_block = KANFeedForwardBlock(d_model, ff_dims, device=device)\n",
    "        if i == N-1:\n",
    "            decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, kan_block, dropout, is_kan=True)\n",
    "        else:\n",
    "            decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, ff_block, dropout, is_kan=False)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(ff_dims[-1], nn.ModuleList(decoder_blocks))\n",
    "    projection_layer = ProjectionLayer(ff_dims[-1], tgt_vocab_size)\n",
    "    \n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    for _, p in transformer.named_parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "466cec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolicQEDTokenizer:\n",
    "    def __init__(self, df=None, index_token_pool_size=100, special_symbols=None, unk_idx=1, to_replace=True):\n",
    "        self.amps = df.amp.tolist() if df is not None else None\n",
    "        self.sqamps = df.sqamp.tolist() if df is not None else None\n",
    "        if index_token_pool_size < 50:\n",
    "            warnings.warn(f\"Index token pool size ({index_token_pool_size}) may be insufficient. Consider using at least 50-100 tokens for symbolic tasks.\", UserWarning)\n",
    "        self.index_pool = [f\"INDEX_{i}\" for i in range(index_token_pool_size)]\n",
    "        self.particle_index_pool = [f\"PINDEX_{i}\" for i in range(index_token_pool_size)]\n",
    "        self.special_symbols = special_symbols or [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\", \"<SEP>\"]\n",
    "        self.unk_idx = unk_idx\n",
    "        self.to_replace = to_replace\n",
    "        self.pattern_underscore_curly = re.compile(r'\\b[\\w]+(?:_[\\w]+)*_{')\n",
    "        self.pattern_mass = re.compile(r'\\bm_([a-z]+)\\b')\n",
    "        self.pattern_mandelstam = re.compile(r'\\bs_(\\d{2,})\\b')\n",
    "        self.pattern_momentum = re.compile(r'\\bp_(\\d+)\\b')\n",
    "        self.pattern_single_s = re.compile(r'\\bs_(\\d+)\\b(?!\\d)')\n",
    "        self.pattern_exponent = re.compile(r'\\^(\\w+|\\([^)]+\\))')\n",
    "        self.pattern_special = re.compile(r'_([uv])|\\\\(\\w+_\\d+|\\w+\\b)')\n",
    "        self.pattern_num_123 = re.compile(r'\\b(?![psijkl]_)(?!MOMENTUM_)(?!MASS_)(?!P_)(?!S_)(?!MANDELSTAM_)\\w+_\\d+\\b')\n",
    "        self.pattern_particle = re.compile(r'(?P<prefix>\\b(?:\\w+_)?)?(?P<target>[ijkl]_\\d+\\b)')\n",
    "\n",
    "    def preprocess_expression(self, expr):\n",
    "        expr = expr.replace(' * ', '*').replace(' / ', '/').replace(' ^ ', '^')\n",
    "        expr = expr.replace(' + ', '+').replace(' - ', '-')\n",
    "        expr = expr.replace(\"+-\", \"-\")\n",
    "        expr = expr.replace(\"-+\", \"-\")\n",
    "        expr = ' '.join(expr.split())\n",
    "        expr = expr.replace('me', 'm_e')\n",
    "        return expr\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_whitespace(expression: str) -> str:\n",
    "        return re.sub(r'\\s+', '', expression)\n",
    "\n",
    "    def protect_structures(self, ampl: str) -> Tuple[str, List[str]]:\n",
    "        protected = []\n",
    "        return ampl, protected\n",
    "\n",
    "    def physics_aware_replace(self, ampl: str, is_source: bool = True) -> str:\n",
    "        ampl = self.remove_whitespace(ampl)\n",
    "        ampl = re.sub(r'\\bi\\b(?!\\w)', 'I_UNIT', ampl)\n",
    "        ampl = re.sub(r'\\be\\b(?=\\^|[+\\-*/()| ])', 'E_CHARGE', ampl)\n",
    "        ampl = ampl.replace('reg_prop', 'REG_PROP')\n",
    "        ampl = self.pattern_mandelstam.sub(r'MANDELSTAM_\\1', ampl)\n",
    "        ampl = self.pattern_momentum.sub(r'P_\\1', ampl)\n",
    "        ampl = self.pattern_single_s.sub(r'S_\\1', ampl)\n",
    "        ampl = ampl.replace('(*)', 'CONJ')\n",
    "        return ampl\n",
    "\n",
    "    def replace_indices(self, ampl: str, is_source: bool = True) -> str:\n",
    "        if not self.to_replace:\n",
    "            return ampl\n",
    "        index_pool = iter(self.index_pool)\n",
    "        particle_index_pool = iter(self.particle_index_pool)\n",
    "        index_pool_set = set(self.index_pool) if is_source else set()\n",
    "\n",
    "        ampl = self.pattern_mandelstam.sub(lambda m: f'MANDELSTAM_{m.group(1)}', ampl)\n",
    "\n",
    "        def get_unique_matches(pattern):\n",
    "            matches = list(OrderedDict.fromkeys(pattern.findall(ampl)))\n",
    "            return [m for m in matches if m not in index_pool_set]\n",
    "\n",
    "        def replace_particle_tokens():\n",
    "            nonlocal ampl\n",
    "            matches = list(OrderedDict.fromkeys(\n",
    "                m.group('target') for m in sorted(self.pattern_particle.finditer(ampl), key=lambda m: m.start())\n",
    "            ))\n",
    "            try:\n",
    "                mapping = {m: next(particle_index_pool) for m in matches}\n",
    "            except StopIteration:\n",
    "                raise RuntimeError(\"particle_index_pool exhausted. Increase the size of the particle_index_pool.\")\n",
    "            for key in sorted(mapping.keys(), key=len, reverse=True):\n",
    "                ampl = ampl.replace(key, mapping[key])\n",
    "\n",
    "        matches = get_unique_matches(self.pattern_num_123)\n",
    "        try:\n",
    "            for match in matches:\n",
    "                ampl = ampl.replace(match, next(index_pool))\n",
    "        except StopIteration:\n",
    "            raise RuntimeError(\"index_pool exhausted. Increase pool size.\")\n",
    "        replace_particle_tokens()\n",
    "        return ampl\n",
    "\n",
    "    def tokenize_expression(self, ampl: str, protected: List[str], is_source: bool = True) -> List[str]:\n",
    "        ampl = ampl.replace('\\\\\\\\', '\\\\')\n",
    "        def replace_special(match):\n",
    "            if match.group(1):\n",
    "                return f' _ {match.group(1)} '\n",
    "            elif match.group(2):\n",
    "                return f' \\\\ {match.group(2)} '\n",
    "        ampl = self.pattern_special.sub(replace_special, ampl)\n",
    "        if is_source:\n",
    "            ampl = self.pattern_underscore_curly.sub(lambda match: f' {match.group(0)} ', ampl)\n",
    "            for symbol in ['{', '}', ',']:\n",
    "                ampl = ampl.replace(symbol, f' {symbol} ')\n",
    "        for symbol in ['/', '+', '-', '*', '(', ')', '^']:\n",
    "            ampl = ampl.replace(symbol, f' {symbol} ')\n",
    "        ampl = self.pattern_exponent.sub(r' ^ \\1 ', ampl)\n",
    "        ampl = ampl.replace('_PINDEX', '_ PINDEX').replace('_INDEX', '_ INDEX')\n",
    "        ampl = ampl.replace('REG_PROP', ' reg_prop ')\n",
    "        ampl = re.sub(r' +', ' ', ampl).strip()\n",
    "        tokens = [token for token in ampl.split(' ') if token]\n",
    "        final_tokens = []\n",
    "        for token in tokens:\n",
    "            if token.startswith('PROTECTED_'):\n",
    "                try:\n",
    "                    idx = int(token.split('_')[1])\n",
    "                    final_tokens.append(protected[idx])\n",
    "                except (IndexError, ValueError):\n",
    "                    final_tokens.append(token)\n",
    "            else:\n",
    "                final_tokens.append(token)\n",
    "        return final_tokens\n",
    "\n",
    "    def src_tokenize(self, ampl: str) -> List[str]:\n",
    "        try:\n",
    "            ampl = self.preprocess_expression(ampl)\n",
    "            ampl, protected = self.protect_structures(ampl)\n",
    "            ampl = self.physics_aware_replace(ampl, is_source=True)\n",
    "            ampl = self.replace_indices(ampl, is_source=True)\n",
    "            return self.tokenize_expression(ampl, protected, is_source=True)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Source tokenization failed for '{ampl}': {e}\")\n",
    "            return [self.special_symbols[self.unk_idx]]\n",
    "\n",
    "    def tgt_tokenize(self, sqampl: str) -> List[str]:\n",
    "        try:\n",
    "            sqampl = self.preprocess_expression(sqampl)\n",
    "            sqampl, protected = self.protect_structures(sqampl)\n",
    "            sqampl = self.physics_aware_replace(sqampl, is_source=False)\n",
    "            sqampl = self.replace_indices(sqampl, is_source=False)\n",
    "            return self.tokenize_expression(sqampl, protected, is_source=False)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Target tokenization failed for '{sqampl}': {e}\")\n",
    "            return [self.special_symbols[self.unk_idx]]\n",
    "\n",
    "    def build_src_vocab(self) -> set:\n",
    "        if self.amps is None:\n",
    "            return set()\n",
    "        vocab_set = set()\n",
    "        start_time = time.time()\n",
    "        for expr in tqdm(self.amps, desc=\"Processing source vocab\"):\n",
    "            vocab_set.update(self.src_tokenize(expr))\n",
    "        end_time = time.time()\n",
    "        print(f\"Source vocab built in {end_time - start_time:.2f} seconds, size: {len(vocab_set)}\")\n",
    "        return vocab_set\n",
    "\n",
    "    def build_tgt_vocab(self) -> set:\n",
    "        if self.sqamps is None:\n",
    "            return set()\n",
    "        vocab_set = set()\n",
    "        start_time = time.time()\n",
    "        for expr in tqdm(self.sqamps, desc=\"Processing target vocab\"):\n",
    "            vocab_set.update(self.tgt_tokenize(expr))\n",
    "        end_time = time.time()\n",
    "        print(f\"Target vocab built in {end_time - start_time:.2f} seconds, size: {len(vocab_set)}\")\n",
    "        return vocab_set\n",
    "\n",
    "class SymbolicVocab:\n",
    "    def __init__(self, tokens: set, special_symbols: list, bos_idx: int, pad_idx: int, eos_idx: int, unk_idx: int, sep_idx: int):\n",
    "        self.token_list = special_symbols + sorted(list(tokens))\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.token_list)}\n",
    "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
    "        self.unk_idx = unk_idx\n",
    "        self.pad_idx = pad_idx\n",
    "        self.bos_idx = bos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.sep_idx = sep_idx\n",
    "        self.unk_tok = special_symbols[unk_idx]\n",
    "        self.pad_tok = special_symbols[pad_idx]\n",
    "        self.bos_tok = special_symbols[bos_idx]\n",
    "        self.eos_tok = special_symbols[eos_idx]\n",
    "        self.sep_tok = special_symbols[sep_idx]\n",
    "\n",
    "    def encode(self, tokens: list) -> list:\n",
    "        return [self.token_to_idx.get(token, self.unk_idx) for token in tokens]\n",
    "\n",
    "    def decode(self, indices: list, include_special_tokens: bool = True) -> list:\n",
    "        if include_special_tokens:\n",
    "            return [self.idx_to_token.get(idx, self.unk_tok) for idx in indices]\n",
    "        return [self.idx_to_token.get(idx, self.unk_tok) for idx in indices \n",
    "                if idx not in {self.pad_idx, self.bos_idx, self.eos_idx, self.sep_idx}]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.token_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, int):\n",
    "            return self.idx_to_token.get(item, self.unk_tok)\n",
    "        return self.token_to_idx.get(item, self.unk_idx)\n",
    "\n",
    "class QEDDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        start_time = time.time()\n",
    "        self.src_vocab = SymbolicVocab(\n",
    "            tokens=tokenizer.build_src_vocab(),\n",
    "            special_symbols=tokenizer.special_symbols,\n",
    "            bos_idx=2,\n",
    "            pad_idx=0,\n",
    "            eos_idx=3,\n",
    "            unk_idx=1,\n",
    "            sep_idx=4\n",
    "        )\n",
    "        self.tgt_vocab = SymbolicVocab(\n",
    "            tokens=tokenizer.build_tgt_vocab(),\n",
    "            special_symbols=tokenizer.special_symbols,\n",
    "            bos_idx=2,\n",
    "            pad_idx=0,\n",
    "            eos_idx=3,\n",
    "            unk_idx=1,\n",
    "            sep_idx=4\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        print(f\"Dataset initialized in {end_time - start_time:.2f} seconds, src_vocab_size: {len(self.src_vocab)}, tgt_vocab_size: {len(self.tgt_vocab)}\")\n",
    "        if len(self.src_vocab) == 5 or len(self.tgt_vocab) == 5:\n",
    "            warnings.warn(\"Vocabulary size is minimal (only special tokens). Check dataset or tokenization.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = str(self.data.iloc[idx][\"amp\"])\n",
    "        trg = str(self.data.iloc[idx][\"sqamp\"])\n",
    "        src_tokens = self.tokenizer.src_tokenize(src)\n",
    "        trg_tokens = self.tokenizer.tgt_tokenize(trg)\n",
    "        src_ids = self.src_vocab.encode(src_tokens)\n",
    "        trg_ids = self.tgt_vocab.encode(trg_tokens)\n",
    "        src_ids = src_ids[:self.max_length] + [self.src_vocab.pad_idx] * (self.max_length - len(src_ids))\n",
    "        trg_ids = trg_ids[:self.max_length] + [self.tgt_vocab.pad_idx] * (self.max_length - len(trg_ids))\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(src_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(trg_ids, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbe2073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in 0.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing source vocab: 100%|██████████| 99/99 [00:00<00:00, 2863.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocab built in 0.04 seconds, size: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing target vocab: 100%|██████████| 99/99 [00:00<00:00, 5237.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target vocab built in 0.02 seconds, size: 45\n",
      "Tokenizer initialized in 0.07 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing source vocab: 100%|██████████| 99/99 [00:00<00:00, 3037.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocab built in 0.04 seconds, size: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing target vocab: 100%|██████████| 99/99 [00:00<00:00, 6180.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target vocab built in 0.02 seconds, size: 45\n",
      "Dataset initialized in 0.05 seconds, src_vocab_size: 83, tgt_vocab_size: 50\n",
      "Data loaders prepared in 0.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized in 0.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DecoderKAN\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Epoch 1/10 (Train): 100%|██████████| 5/5 [04:31<00:00, 54.20s/it]\n",
      "Epoch 1/10 (Val): 100%|██████████| 2/2 [00:17<00:00,  8.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 completed in 271.02 seconds: Train Loss: 12.3954, Train Acc: 17.85%, Val Loss: 8.0607, Val Acc: 35.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 (Train): 100%|██████████| 5/5 [03:58<00:00, 47.79s/it]\n",
      "Epoch 2/10 (Val): 100%|██████████| 2/2 [00:17<00:00,  8.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 completed in 238.97 seconds: Train Loss: 6.5935, Train Acc: 37.81%, Val Loss: 4.4753, Val Acc: 39.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 (Train): 100%|██████████| 5/5 [04:10<00:00, 50.03s/it]\n",
      "Epoch 3/10 (Val): 100%|██████████| 2/2 [00:17<00:00,  8.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 completed in 250.15 seconds: Train Loss: 3.4491, Train Acc: 47.18%, Val Loss: 2.7489, Val Acc: 49.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 (Train): 100%|██████████| 5/5 [04:05<00:00, 49.01s/it]\n",
      "Epoch 4/10 (Val): 100%|██████████| 2/2 [00:16<00:00,  8.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 completed in 245.08 seconds: Train Loss: 2.2878, Train Acc: 54.16%, Val Loss: 1.6858, Val Acc: 55.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 (Train): 100%|██████████| 5/5 [03:58<00:00, 47.68s/it]\n",
      "Epoch 5/10 (Val): 100%|██████████| 2/2 [00:17<00:00,  8.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 completed in 238.41 seconds: Train Loss: 1.6033, Train Acc: 57.10%, Val Loss: 1.4788, Val Acc: 60.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 (Train): 100%|██████████| 5/5 [04:07<00:00, 49.53s/it]\n",
      "Epoch 6/10 (Val): 100%|██████████| 2/2 [00:17<00:00,  8.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 completed in 247.67 seconds: Train Loss: 1.3899, Train Acc: 60.85%, Val Loss: 1.2959, Val Acc: 62.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 (Train): 100%|██████████| 5/5 [04:55<00:00, 59.14s/it]\n",
      "Epoch 7/10 (Val): 100%|██████████| 2/2 [00:17<00:00,  8.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 completed in 295.72 seconds: Train Loss: 1.2236, Train Acc: 62.96%, Val Loss: 1.1722, Val Acc: 64.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 (Train): 100%|██████████| 5/5 [05:38<00:00, 67.72s/it]\n",
      "Epoch 8/10 (Val): 100%|██████████| 2/2 [00:17<00:00,  8.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 completed in 338.59 seconds: Train Loss: 1.0912, Train Acc: 65.17%, Val Loss: 1.0764, Val Acc: 64.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 (Train): 100%|██████████| 5/5 [05:25<00:00, 65.12s/it]\n",
      "Epoch 9/10 (Val): 100%|██████████| 2/2 [00:23<00:00, 11.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 completed in 325.58 seconds: Train Loss: 1.0248, Train Acc: 66.46%, Val Loss: 1.0011, Val Acc: 66.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 (Train): 100%|██████████| 5/5 [05:13<00:00, 62.78s/it]\n",
      "Epoch 10/10 (Val): 100%|██████████| 2/2 [00:17<00:00,  8.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 completed in 313.90 seconds: Train Loss: 0.9591, Train Acc: 67.87%, Val Loss: 1.0955, Val Acc: 65.78%\n",
      "Inference completed in 109.10 seconds\n",
      "Input: 1/9*i*e^2*(p_2_\\INDEX_0*gamma_{+\\INDEX_0,INDEX_1,INDEX_2}*gamma_{\\INDEX_3,INDEX_4,INDEX_1}*gamma_{\\INDEX_5,INDEX_2,INDEX_6}*A_{MOMENTUM_0,+\\INDEX_5}(p_3)^(*)*A_{MOMENTUM_1,+\\INDEX_3}(p_4)^(*)*b_{MOMENTUM_2,INDEX_6}(p_2)_u*b_{MOMENTUM_3,INDEX_4}(p_1)_v^(*)+-p_3_\\INDEX_0*gamma_{+\\INDEX_0,INDEX_7,INDEX_8}*gamma_{\\INDEX_3,INDEX_9,INDEX_7}*gamma_{\\INDEX_5,INDEX_8,INDEX_10}*A_{MOMENTUM_0,+\\INDEX_5}(p_3)^(*)*A_{MOMENTUM_1,+\\INDEX_3}(p_4)^(*)*b_{MOMENTUM_2,INDEX_10}(p_2)_u*b_{MOMENTUM_3,INDEX_9}(p_1)_v^(*)+m_b*gamma_{\\INDEX_3,INDEX_11,INDEX_12}*gamma_{\\INDEX_5,INDEX_12,INDEX_13}*A_{MOMENTUM_0,+\\INDEX_5}(p_3)^(*)*A_{MOMENTUM_1,+\\INDEX_3}(p_4)^(*)*b_{MOMENTUM_2,INDEX_13}(p_2)_u*b_{MOMENTUM_3,INDEX_11}(p_1)_v^(*))/(m_b^2+-s_22+2*s_23+-s_33+-reg_prop)\n",
      "Output IDs: [[2, 10, 23, 7, 25, 37, 19, 7, 5, 20, 7, 40, 37, 14, 7, 40, 37, 14, 9, 13, 6, 9, 13, 6, 9, 31, 7, 40, 37, 14, 9, 27, 8, 14, 9, 34, 9, 48, 6, 37, 5, 26, 7, 25, 37, 14, 6, 9, 14, 6, 10, 23, 7, 25, 37, 14, 7, 25, 37, 14, 9, 11, 10, 5, 26, 7, 40, 7, 40, 37, 14, 9, 48, 6, 10, 5, 40, 37, 14, 9, 27, 8, 14, 9, 34, 9, 34, 9, 26, 7, 25, 37, 14, 9, 26, 7, 40, 37, 14, 9, 31, 8, 14, 9, 31, 8, 14, 9, 27, 8, 14, 9, 27, 8, 14, 9, 27, 8, 14, 9, 48, 6, 10, 5, 41, 37, 14, 9, 27, 8, 14, 9, 31, 8, 14, 9, 48, 6, 10, 5, 40, 37, 14, 9, 27, 8, 14, 9, 31, 8, 14, 9, 34, 9, 48, 6, 10, 5, 26, 7, 29, 9, 31, 8, 14, 9, 34, 9, 34, 9, 48, 6, 10, 23, 7, 25, 37, 14, 7, 25, 37, 14, 9, 26, 7, 25, 37, 14, 9, 31, 8, 14, 9, 31, 8, 14, 9, 31, 8, 14, 7, 29, 9, 31, 8, 14, 9, 31, 8, 14, 9, 31, 8, 14, 9, 27, 8, 14, 9, 34, 9, 34, 9, 34, 9, 31, 8, 14, 9, 31, 8, 14, 9, 27, 8, 14, 9, 34, 9, 48, 6, 10, 5, 41, 37, 14, 9, 27, 8, 14, 9, 34, 9, 48, 6, 10, 5, 40, 37, 14, 9, 27, 8, 14, 9, 27, 8, 14, 9, 27, 8, 14, 9, 34, 9, 34, 9, 48, 6, 10, 5, 40, 37, 14, 9, 27, 8, 14, 9, 34, 9, 34, 9, 48, 6, 10, 5, 40, 37, 14, 9]]\n",
      "Output: <BOS>/81*E_CHARGE^4*(64*m_b^2*m_b^2-16)-16)-MANDELSTAM_22*m_b^2-MANDELSTAM_11+2-MANDELSTAM_33-reg_prop)^(I_UNIT*E_CHARGE^2)-2)/81*E_CHARGE^2*E_CHARGE^2-1/(I_UNIT*m_b*m_b^2-reg_prop)/(m_b^2-MANDELSTAM_11+2-MANDELSTAM_33-MANDELSTAM_33-I_UNIT*E_CHARGE^2-I_UNIT*m_b^2-MANDELSTAM_22+2-MANDELSTAM_22+2-MANDELSTAM_11+2-MANDELSTAM_11+2-MANDELSTAM_11+2-reg_prop)/(m_c^2-MANDELSTAM_11+2-MANDELSTAM_22+2-reg_prop)/(m_b^2-MANDELSTAM_11+2-MANDELSTAM_22+2-MANDELSTAM_33-reg_prop)/(I_UNIT*MANDELSTAM_13-MANDELSTAM_22+2-MANDELSTAM_33-MANDELSTAM_33-reg_prop)/81*E_CHARGE^2*E_CHARGE^2-I_UNIT*E_CHARGE^2-MANDELSTAM_22+2-MANDELSTAM_22+2-MANDELSTAM_22+2*MANDELSTAM_13-MANDELSTAM_22+2-MANDELSTAM_22+2-MANDELSTAM_22+2-MANDELSTAM_11+2-MANDELSTAM_33-MANDELSTAM_33-MANDELSTAM_33-MANDELSTAM_22+2-MANDELSTAM_22+2-MANDELSTAM_11+2-MANDELSTAM_33-reg_prop)/(m_c^2-MANDELSTAM_11+2-MANDELSTAM_33-reg_prop)/(m_b^2-MANDELSTAM_11+2-MANDELSTAM_11+2-MANDELSTAM_11+2-MANDELSTAM_33-MANDELSTAM_33-reg_prop)/(m_b^2-MANDELSTAM_11+2-MANDELSTAM_33-MANDELSTAM_33-reg_prop)/(m_b^2-\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "N = 3\n",
    "h = 8\n",
    "dropout = 0.1\n",
    "d_ff = 4096\n",
    "ff_dims = [8192]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_length = 300  \n",
    "epochs = 10\n",
    "lr = 1e-4\n",
    "batch_size = 16\n",
    "\n",
    "# Load Data\n",
    "start_time = time.time()\n",
    "data_df = pd.read_csv(r'D:\\DecoderKAN\\QED_data\\test-flow.csv')\n",
    "print(f\"Data loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Tokenizer\n",
    "start_time = time.time()\n",
    "tokenizer = SymbolicQEDTokenizer(df=data_df, index_token_pool_size=100, special_symbols=[\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\", \"<SEP>\"], to_replace=True)\n",
    "src_vocab_size = len(tokenizer.build_src_vocab()) + 5  # +5 for special tokens\n",
    "tgt_vocab_size = len(tokenizer.build_tgt_vocab()) + 5  # +5 for special tokens\n",
    "print(f\"Tokenizer initialized in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "start_time = time.time()\n",
    "dataset = QEDDataset(data_df, tokenizer, max_length)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = Subset(dataset, range(train_size)), Subset(dataset, range(train_size, len(dataset)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "print(f\"Data loaders prepared in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Build Model\n",
    "start_time = time.time()\n",
    "model = build_kanformer(src_vocab_size, tgt_vocab_size, max_length, max_length, d_model, N, h, dropout, d_ff, ff_dims, device)\n",
    "model.to(device)\n",
    "print(f\"Model initialized in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Training Loop\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # pad_idx = 0\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    start_time = time.time()\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} (Train)\"):\n",
    "        src = batch[\"input_ids\"].to(device)\n",
    "        trg = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, src_len, 1]\n",
    "        tgt_len = trg[:, :-1].size(1)\n",
    "        tgt_pad_mask = (trg[:, :-1] != 0).unsqueeze(1).unsqueeze(3)\n",
    "        tgt_sub_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=device)).bool()\n",
    "        tgt_mask = tgt_pad_mask & tgt_sub_mask.unsqueeze(0).unsqueeze(0)\n",
    "        enc_output = model.encode(src, src_mask)\n",
    "        dec_output = model.decode(enc_output, src_mask, trg[:, :-1], tgt_mask)\n",
    "        logits = model.project(dec_output)\n",
    "        output = logits.view(-1, logits.size(-1))\n",
    "        target = trg[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        preds = torch.argmax(output, dim=-1)\n",
    "        mask = target != 0\n",
    "        train_correct += (preds[mask] == target[mask]).sum().item()\n",
    "        train_total += mask.sum().item()\n",
    "    end_time = time.time()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = train_correct / train_total if train_total > 0 else 0\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} (Val)\"):\n",
    "            src = batch[\"input_ids\"].to(device)\n",
    "            trg = batch[\"labels\"].to(device)\n",
    "            src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "            tgt_len = trg[:, :-1].size(1)\n",
    "            tgt_pad_mask = (trg[:, :-1] != 0).unsqueeze(1).unsqueeze(3)\n",
    "            tgt_sub_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=device)).bool()\n",
    "            tgt_mask = tgt_pad_mask & tgt_sub_mask.unsqueeze(0).unsqueeze(0)\n",
    "            enc_output = model.encode(src, src_mask)\n",
    "            dec_output = model.decode(enc_output, src_mask, trg[:, :-1], tgt_mask)\n",
    "            logits = model.project(dec_output)\n",
    "            output = logits.view(-1, logits.size(-1))\n",
    "            target = trg[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(output, dim=-1)\n",
    "            mask = target != 0\n",
    "            val_correct += (preds[mask] == target[mask]).sum().item()\n",
    "            val_total += mask.sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = val_correct / val_total if val_total > 0 else 0\n",
    "    print(f\"Epoch {epoch+1}/{epochs} completed in {end_time - start_time:.2f} seconds: \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2%}, \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2%}\")\n",
    "# Inference\n",
    "model.eval()\n",
    "start_time = time.time()\n",
    "test_expr = r\"1/9*i*e^2*(p_2_\\INDEX_0*gamma_{+\\INDEX_0,INDEX_1,INDEX_2}*gamma_{\\INDEX_3,INDEX_4,INDEX_1}*gamma_{\\INDEX_5,INDEX_2,INDEX_6}*A_{MOMENTUM_0,+\\INDEX_5}(p_3)^(*)*A_{MOMENTUM_1,+\\INDEX_3}(p_4)^(*)*b_{MOMENTUM_2,INDEX_6}(p_2)_u*b_{MOMENTUM_3,INDEX_4}(p_1)_v^(*)+-p_3_\\INDEX_0*gamma_{+\\INDEX_0,INDEX_7,INDEX_8}*gamma_{\\INDEX_3,INDEX_9,INDEX_7}*gamma_{\\INDEX_5,INDEX_8,INDEX_10}*A_{MOMENTUM_0,+\\INDEX_5}(p_3)^(*)*A_{MOMENTUM_1,+\\INDEX_3}(p_4)^(*)*b_{MOMENTUM_2,INDEX_10}(p_2)_u*b_{MOMENTUM_3,INDEX_9}(p_1)_v^(*)+m_b*gamma_{\\INDEX_3,INDEX_11,INDEX_12}*gamma_{\\INDEX_5,INDEX_12,INDEX_13}*A_{MOMENTUM_0,+\\INDEX_5}(p_3)^(*)*A_{MOMENTUM_1,+\\INDEX_3}(p_4)^(*)*b_{MOMENTUM_2,INDEX_13}(p_2)_u*b_{MOMENTUM_3,INDEX_11}(p_1)_v^(*))/(m_b^2+-s_22+2*s_23+-s_33+-reg_prop)\"\n",
    "src_tokens = tokenizer.src_tokenize(test_expr)\n",
    "src_ids = torch.tensor([dataset.src_vocab.encode(src_tokens)], device=device)\n",
    "src_mask = (src_ids != 0).unsqueeze(1).unsqueeze(2)\n",
    "enc_output = model.encode(src_ids, src_mask)\n",
    "trg = torch.full((1, 1), 2, dtype=torch.long, device=device)  # Start with BOS token\n",
    "for _ in range(max_length):\n",
    "    tgt_mask = (trg != 0).unsqueeze(1).unsqueeze(3) & torch.tril(torch.ones(trg.size(1), trg.size(1), device=device)).bool()\n",
    "    dec_output = model.decode(enc_output, src_mask, trg, tgt_mask)\n",
    "    logits = model.project(dec_output[:, -1])\n",
    "    pred = torch.argmax(logits, dim=-1).unsqueeze(1)\n",
    "    trg = torch.cat([trg, pred], dim=1)\n",
    "    if pred.item() == 3:  # EOS token\n",
    "        break\n",
    "decoded = dataset.tgt_vocab.decode(trg[0].tolist())\n",
    "print(f\"Inference completed in {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"Input: {test_expr}\")\n",
    "print(f\"Output IDs: {trg.tolist()}\")\n",
    "print(f\"Output: {''.join(decoded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7bdde3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
