{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e83d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set, Optional, Tuple\n",
    "from collections import OrderedDict\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sympy as sp\n",
    "import warnings\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SymbolicQEDTokenizer:\n",
    "    def __init__(self, df: Optional[pd.DataFrame] = None, \n",
    "                 index_token_pool_size: int = 100,\n",
    "                 special_symbols: List[str] = None,\n",
    "                 unk_idx: int = 1,\n",
    "                 to_replace: bool = False):\n",
    "        self.amps = df.amp.tolist() if df is not None else None\n",
    "        self.sqamps = df.sqamp.tolist() if df is not None else None\n",
    "        \n",
    "        if index_token_pool_size < 50:\n",
    "            warnings.warn(\n",
    "                f\"Index token pool size ({index_token_pool_size}) may be insufficient. \"\n",
    "                \"Consider using at least 50-100 tokens for symbolic tasks.\",\n",
    "                UserWarning\n",
    "            )\n",
    "\n",
    "        self.index_pool = [f\"INDEX_{i}\" for i in range(index_token_pool_size)]\n",
    "        self.particle_index_pool = [f\"PINDEX_{i}\" for i in range(index_token_pool_size)]\n",
    "        self.special_symbols = special_symbols or [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\", \"<SEP>\"]\n",
    "        self.unk_idx = unk_idx\n",
    "        self.to_replace = to_replace\n",
    "        self.pattern_underscore_curly = re.compile(r'\\b[\\w]+(?:_[\\w]+)*_{')  # e.g., gamma_{, b_{MOMENTUM_0\n",
    "        self.pattern_mass = re.compile(r'\\bm_([a-z]+)\\b')  # e.g., m_c\n",
    "        self.pattern_mandelstam = re.compile(r'\\bs_(\\d{2,})\\b')  # e.g., s_12\n",
    "        self.pattern_momentum = re.compile(r'\\bp_(\\d+)\\b')  # e.g., p_1\n",
    "        self.pattern_single_s = re.compile(r'\\bs_(\\d+)\\b(?!\\d)')  # e.g., s_1\n",
    "        self.pattern_exponent = re.compile(r'\\^(\\w+|\\([^)]+\\))')  # e.g., ^2, ^(*)\n",
    "        self.pattern_special = re.compile(r'_([uv])|\\\\(\\w+_\\d+|\\w+\\b)')  # Spinor suffixes and LaTeX indices\n",
    "        self.pattern_num_123 = re.compile(r'\\b(?![psijkl]_)(?!MOMENTUM_)(?!MASS_)(?!P_)(?!S_)(?!MANDELSTAM_)\\w+_\\d+\\b')  # Non-physics indices\n",
    "        self.pattern_particle = re.compile(r'(?P<prefix>\\b(?:\\w+_)?)?(?P<target>[ijkl]_\\d+\\b)')  # Particle indices\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_whitespace(expression: str) -> str:\n",
    "        return re.sub(r'\\s+', '', expression)\n",
    "\n",
    "    def protect_structures(self, ampl: str) -> Tuple[str, List[str]]:\n",
    "        protected = []\n",
    "        return ampl, protected\n",
    "\n",
    "    def physics_aware_replace(self, ampl: str, is_source: bool = True) -> str:\n",
    "        ampl = self.remove_whitespace(ampl)\n",
    "        ampl = re.sub(r'\\bi\\b(?!\\w)', 'I_UNIT', ampl)\n",
    "        ampl = re.sub(r'\\be\\b(?=\\^|[+\\-*/()| ])', 'E_CHARGE', ampl)  # Handle e^X or standalone e\n",
    "        ampl = ampl.replace('reg_prop', 'REG_PROP')\n",
    "        # ampl = self.pattern_mass.sub(r'MASS_\\1', ampl)\n",
    "        ampl = self.pattern_mandelstam.sub(r'MANDELSTAM_\\1', ampl)\n",
    "        ampl = self.pattern_momentum.sub(r'P_\\1', ampl)\n",
    "        ampl = self.pattern_single_s.sub(r'S_\\1', ampl)\n",
    "        ampl = ampl.replace('(*)', 'CONJ')\n",
    "        return ampl\n",
    "\n",
    "    def replace_indices(self, ampl: str, is_source: bool = True) -> str:\n",
    "        if not self.to_replace:\n",
    "            return ampl\n",
    "        index_pool = iter(self.index_pool)\n",
    "        particle_index_pool = iter(self.particle_index_pool)\n",
    "        index_pool_set = set(self.index_pool) if is_source else set()\n",
    "\n",
    "        ampl = self.pattern_mandelstam.sub(lambda m: f'MANDELSTAM_{m.group(1)}', ampl)\n",
    "\n",
    "        def get_unique_matches(pattern):\n",
    "            matches = list(OrderedDict.fromkeys(pattern.findall(ampl)))\n",
    "            return [m for m in matches if m not in index_pool_set]\n",
    "\n",
    "        def replace_particle_tokens():\n",
    "            nonlocal ampl\n",
    "            matches = list(OrderedDict.fromkeys(\n",
    "                m.group('target') for m in sorted(self.pattern_particle.finditer(ampl), key=lambda m: m.start())\n",
    "            ))\n",
    "            try:\n",
    "                mapping = {m: next(particle_index_pool) for m in matches}\n",
    "            except StopIteration:\n",
    "                raise RuntimeError(\"particle_index_pool exhausted. Increase the size of the particle_index_pool.\")\n",
    "            for key in sorted(mapping.keys(), key=len, reverse=True):\n",
    "                ampl = ampl.replace(key, mapping[key])\n",
    "\n",
    "        matches = get_unique_matches(self.pattern_num_123)\n",
    "        try:\n",
    "            for match in matches:\n",
    "                ampl = ampl.replace(match, next(index_pool))\n",
    "        except StopIteration:\n",
    "            raise RuntimeError(\"index_pool exhausted. Increase pool size.\")\n",
    "        replace_particle_tokens()\n",
    "        return ampl\n",
    "\n",
    "    def tokenize_expression(self, ampl: str, protected: List[str], is_source: bool = True) -> List[str]:\n",
    "        ampl = ampl.replace('\\\\\\\\', '\\\\')\n",
    "        def replace_special(match):\n",
    "            if match.group(1):  # Spinor suffix (_u or _v)\n",
    "                return f' _ {match.group(1)} '\n",
    "            elif match.group(2):  # LaTeX index (\\INDEX_0 or \\+)\n",
    "                return f' \\\\ {match.group(2)} '\n",
    "        ampl = self.pattern_special.sub(replace_special, ampl)\n",
    "        if is_source:\n",
    "            ampl = self.pattern_underscore_curly.sub(lambda match: f' {match.group(0)} ', ampl)\n",
    "            for symbol in ['{', '}', ',']:\n",
    "                ampl = ampl.replace(symbol, f' {symbol} ')\n",
    "        # Standard operator splitting\n",
    "        for symbol in ['/', '+', '-', '*', '(', ')', '^']:\n",
    "            ampl = ampl.replace(symbol, f' {symbol} ')\n",
    "        ampl = self.pattern_exponent.sub(r' ^ \\1 ', ampl)\n",
    "        ampl = ampl.replace('_PINDEX', '_ PINDEX').replace('_INDEX', '_ INDEX')\n",
    "        ampl = ampl.replace('REG_PROP', ' reg_prop ')\n",
    "        ampl = re.sub(r' +', ' ', ampl).strip()\n",
    "        tokens = [token for token in ampl.split(' ') if token]\n",
    "        final_tokens = []\n",
    "        for token in tokens:\n",
    "            if token.startswith('PROTECTED_'):\n",
    "                try:\n",
    "                    idx = int(token.split('_')[1])\n",
    "                    final_tokens.append(protected[idx])\n",
    "                except (IndexError, ValueError):\n",
    "                    final_tokens.append(token)\n",
    "            else:\n",
    "                final_tokens.append(token)\n",
    "        return final_tokens\n",
    "\n",
    "    def src_tokenize(self, ampl: str) -> List[str]:\n",
    "        try:\n",
    "            ampl, protected = self.protect_structures(ampl)\n",
    "            ampl = self.physics_aware_replace(ampl, is_source=True)\n",
    "            ampl = self.replace_indices(ampl, is_source=True)\n",
    "            return self.tokenize_expression(ampl, protected, is_source=True)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Source tokenization failed for '{ampl}': {e}\")\n",
    "            return [self.special_symbols[self.unk_idx]]\n",
    "\n",
    "    def tgt_tokenize(self, sqampl: str) -> List[str]:\n",
    "        try:\n",
    "            sqampl, protected = self.protect_structures(sqampl)\n",
    "            sqampl = self.physics_aware_replace(sqampl, is_source=False)\n",
    "            sqampl = self.replace_indices(sqampl, is_source=False)\n",
    "            return self.tokenize_expression(sqampl, protected, is_source=False)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Target tokenization failed for '{sqampl}': {e}\")\n",
    "            return [self.special_symbols[self.unk_idx]]\n",
    "\n",
    "    def build_src_vocab(self) -> Set[str]:\n",
    "        if self.amps is None:\n",
    "            return set()\n",
    "        vocab_set = set()\n",
    "        for expr in tqdm(self.amps, desc=\"Processing source vocab\"):\n",
    "            vocab_set.update(self.src_tokenize(expr))\n",
    "        return vocab_set\n",
    "\n",
    "    def build_tgt_vocab(self) -> Set[str]:\n",
    "        if self.sqamps is None:\n",
    "            return set()\n",
    "        vocab_set = set()\n",
    "        for expr in tqdm(self.sqamps, desc=\"Processing target vocab\"):\n",
    "            vocab_set.update(self.tgt_tokenize(expr))\n",
    "        return vocab_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509c54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SymbolicVocab:\n",
    "    def __init__(self, tokens: Set[str], special_symbols: List[str], \n",
    "                 bos_idx: int, pad_idx: int, eos_idx: int, unk_idx: int, sep_idx: int):\n",
    "        self.token_list = special_symbols + sorted(list(tokens))\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.token_list)}\n",
    "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
    "        self.unk_idx = unk_idx\n",
    "        self.pad_idx = pad_idx\n",
    "        self.bos_idx = bos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.sep_idx = sep_idx\n",
    "        self.unk_tok = special_symbols[unk_idx]\n",
    "        self.pad_tok = special_symbols[pad_idx]\n",
    "        self.bos_tok = special_symbols[bos_idx]\n",
    "        self.eos_tok = special_symbols[eos_idx]\n",
    "        self.sep_tok = special_symbols[sep_idx]\n",
    "\n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        return [self.token_to_idx.get(token, self.unk_idx) for token in tokens]\n",
    "\n",
    "    def decode(self, indices: List[int], include_special_tokens: bool = True) -> List[str]:\n",
    "        if include_special_tokens:\n",
    "            return [self.idx_to_token.get(idx, self.unk_tok) for idx in indices]\n",
    "        return [self.idx_to_token.get(idx, self.unk_tok) for idx in indices \n",
    "                if idx not in {self.pad_idx, self.bos_idx, self.eos_idx, self.sep_idx}]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.token_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, int):\n",
    "            return self.idx_to_token.get(item, self.unk_tok)\n",
    "        return self.token_to_idx.get(item, self.unk_idx)\n",
    "\n",
    "    def tokens(self) -> List[str]:\n",
    "        return self.token_list\n",
    "\n",
    "    def save(self, filepath: str):\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            for token in self.token_list:\n",
    "                f.write(f\"{token}\\n\")\n",
    "        with open(filepath.replace('.txt', '.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.token_to_idx, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def reconstruct_expression(tokens: List[str]) -> str:\n",
    "    expr = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        # Handle negative numbers\n",
    "        if token == '-' and i + 1 < len(tokens) and tokens[i + 1].isdigit():\n",
    "            expr.append(token)\n",
    "        elif token.startswith('MASS_'):\n",
    "            expr.append(f'm_{token[5:]}')\n",
    "        elif token.startswith('MANDELSTAM_'):\n",
    "            expr.append(f's_{token[11:]}')\n",
    "        elif token.startswith('P_'):\n",
    "            expr.append(f'p_{token[2:]}')\n",
    "        elif token == 'I_UNIT':\n",
    "            expr.append('i')\n",
    "        elif token == 'E_CHARGE':\n",
    "            expr.append('e')\n",
    "        elif token == 'REG_PROP':\n",
    "            expr.append('reg_prop')\n",
    "        elif token == 'CONJ':\n",
    "            expr.append('(*)')\n",
    "        elif token.startswith('S_'):\n",
    "            expr.append(f's_{token[2:]}')\n",
    "        # Handle field term prefixes (e.g., gamma_{, +)\n",
    "        elif token.endswith('_{') and i + 1 < len(tokens) and tokens[i + 1] in ['+', '-', '\\\\']:\n",
    "            expr.append(token[:-1])\n",
    "            i += 1\n",
    "            continue\n",
    "        # Handle spinor suffixes (e.g., _, u)\n",
    "        elif token == '_' and i + 1 < len(tokens) and tokens[i + 1] in ['u', 'v']:\n",
    "            expr.append(f'_{tokens[i + 1]}')\n",
    "            i += 1\n",
    "        # Handle LaTeX backslashes (e.g., \\, INDEX_0)\n",
    "        elif token == '\\\\' and i + 1 < len(tokens) and (tokens[i + 1].startswith(('INDEX_', 'MOMENTUM_')) or tokens[i + 1] in ['+', '-']):\n",
    "            expr.append(f'\\\\{tokens[i + 1]}')\n",
    "            i += 1\n",
    "        else:\n",
    "            expr.append(token)\n",
    "        i += 1\n",
    "    return ''.join(expr)\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def parse_qed_expression(expr: str) -> Optional[sp.Expr]:\n",
    "    i, e, m_b, m_c = sp.symbols('i e m_b m_c')\n",
    "    s_12, s_14, s_23, s_13, s_24, s_34 = sp.symbols('s_12 s_14 s_23 s_13 s_24 s_34')\n",
    "    reg_prop = sp.symbols('reg_prop')\n",
    "    \n",
    "    expr = expr.replace('^', '**')\n",
    "    \n",
    "    try:\n",
    "        parsed = sp.sympify(expr, locals={\n",
    "            'i': i,\n",
    "            'e': e,\n",
    "            'm_b': m_b,\n",
    "            'm_c': m_c,\n",
    "            's_12': s_12,\n",
    "            's_14': s_14,\n",
    "            's_23': s_23,\n",
    "            's_13': s_13,\n",
    "            's_24': s_24,\n",
    "            's_34': s_34,\n",
    "            'reg_prop': reg_prop\n",
    "        })\n",
    "        return parsed\n",
    "    except sp.SympifyError as e:\n",
    "        print(f\"SymPy parsing error for expression '{expr}': {e}\")\n",
    "        return None\n",
    "\n",
    "def validate_expression(original: str, tokens: List[str], is_source: bool = True) -> Tuple[bool, bool]:\n",
    "    reconstructed = reconstruct_expression(tokens)\n",
    "    string_match = original == reconstructed\n",
    "    \n",
    "    symbolic_match = False\n",
    "    if not is_source:\n",
    "        orig_parsed = parse_qed_expression(original)\n",
    "        recon_parsed = parse_qed_expression(reconstructed)\n",
    "        if orig_parsed and recon_parsed:\n",
    "            symbolic_match = sp.simplify(orig_parsed - recon_parsed) == 0\n",
    "        else:\n",
    "            print(f\"Symbolic parsing failed for {'original' if not orig_parsed else 'reconstructed'} target expression.\")\n",
    "    \n",
    "    return string_match, symbolic_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d753640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 9952 records.\n"
     ]
    }
   ],
   "source": [
    "special_symbols = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\", \"<SEP>\"]\n",
    "\n",
    "csv_file = r\"D:\\DecoderKAN\\QED_data\\train_data.csv\"\n",
    "try:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Loaded dataset with {len(df)} records.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset not found. Please update the path to QED_data/train_data.csv.\")\n",
    "\n",
    "tokenizer = SymbolicQEDTokenizer(\n",
    "    df=df,\n",
    "    index_token_pool_size=100,\n",
    "    special_symbols=special_symbols,\n",
    "    unk_idx=1,\n",
    "    to_replace=True  # Enable index replacement\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37ff8c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Source Expression ===\n",
      "Source Original: 1/9*i*e^2*gamma_{+\\INDEX_0,INDEX_1,INDEX_2}*gamma_{\\INDEX_0,INDEX_3,INDEX_4}*b_{MOMENTUM_0,INDEX_4}(p_3)_v*b_{MOMENTUM_1,INDEX_3}(p_4)_u^(*)*c_{MOMENTUM_2,INDEX_2}(p_1)_u*c_{MOMENTUM_3,INDEX_1}(p_2)_v^(*)/(m_c^2+s_12+1/2*reg_prop)\n",
      "Source Tokens: ['1', '/', '9', '*', 'I_UNIT', '*', 'E_CHARGE', '^', '2', '*', 'gamma_', '{', '+', '\\\\', 'INDEX_0', ',', 'INDEX_1', ',', 'INDEX_2', '}', '*', 'gamma_', '{', '\\\\', 'INDEX_0', ',', 'INDEX_3', ',', 'INDEX_4', '}', '*', 'b_', '{', 'MOMENTUM_0', ',', 'INDEX_4', '}', '(', 'P_3', ')', '_', 'v', '*', 'b_', '{', 'MOMENTUM_1', ',', 'INDEX_3', '}', '(', 'P_4', ')', '_', 'u', '^', 'CONJ', '*', 'c_', '{', 'MOMENTUM_2', ',', 'INDEX_2', '}', '(', 'P_1', ')', '_', 'u', '*', 'c_', '{', 'MOMENTUM_3', ',', 'INDEX_1', '}', '(', 'P_2', ')', '_', 'v', '^', 'CONJ', '/', '(', 'm_c', '^', '2', '+', 'MANDELSTAM_12', '+', '1', '/', '2', '*', 'reg_prop', ')']\n",
      "Source Reconstructed: 1/9*i*e^2*gamma_{+\\INDEX_0,INDEX_1,INDEX_2}*gamma_{\\INDEX_0,INDEX_3,INDEX_4}*b_{MOMENTUM_0,INDEX_4}(p_3)_v*b_{MOMENTUM_1,INDEX_3}(p_4)_u^(*)*c_{MOMENTUM_2,INDEX_2}(p_1)_u*c_{MOMENTUM_3,INDEX_1}(p_2)_v^(*)/(m_c^2+s_12+1/2*reg_prop)\n",
      "Source String Match: True\n",
      "\n",
      "=== Testing Target Expression ===\n",
      "Target Original: 1/81*e^4*(16*m_b^2*m_c^2 + 8*m_b^2*s_12 + 8*s_14*s_23 + - 8*s_13*s_24 + 8*m_c^2*s_34)*(m_c^2 + s_12 + 1/2*reg_prop)^(-2)\n",
      "Target Tokens: ['1', '/', '81', '*', 'E_CHARGE', '^', '4', '*', '(', '16', '*', 'm_b', '^', '2', '*', 'm_c', '^', '2', '+', '8', '*', 'm_b', '^', '2', '*', 'MANDELSTAM_12', '+', '8', '*', 'MANDELSTAM_14', '*', 'MANDELSTAM_23', '+', '-', '8', '*', 'MANDELSTAM_13', '*', 'MANDELSTAM_24', '+', '8', '*', 'm_c', '^', '2', '*', 'MANDELSTAM_34', ')', '*', '(', 'm_c', '^', '2', '+', 'MANDELSTAM_12', '+', '1', '/', '2', '*', 'reg_prop', ')', '^', '(', '-', '2', ')']\n",
      "Target Reconstructed: 1/81*e^4*(16*m_b^2*m_c^2+8*m_b^2*s_12+8*s_14*s_23+-8*s_13*s_24+8*m_c^2*s_34)*(m_c^2+s_12+1/2*reg_prop)^(-2)\n",
      "Target String Match: False\n",
      "Target Symbolic Equivalence: True\n"
     ]
    }
   ],
   "source": [
    "src_expr = \"1/9*i*e^2*gamma_{+\\\\INDEX_0,INDEX_1,INDEX_2}*gamma_{\\\\INDEX_0,INDEX_3,INDEX_4}*b_{MOMENTUM_0,INDEX_4}(p_3)_v*b_{MOMENTUM_1,INDEX_3}(p_4)_u^(*)*c_{MOMENTUM_2,INDEX_2}(p_1)_u*c_{MOMENTUM_3,INDEX_1}(p_2)_v^(*)/(m_c^2+s_12+1/2*reg_prop)\"\n",
    "tgt_expr = \"1/81*e^4*(16*m_b^2*m_c^2 + 8*m_b^2*s_12 + 8*s_14*s_23 + - 8*s_13*s_24 + 8*m_c^2*s_34)*(m_c^2 + s_12 + 1/2*reg_prop)^(-2)\"\n",
    "\n",
    "print(\"\\n=== Testing Source Expression ===\")\n",
    "src_tokens = tokenizer.src_tokenize(src_expr)\n",
    "src_string_match, _ = validate_expression(src_expr, src_tokens, is_source=True)\n",
    "print(f\"Source Original: {src_expr}\")\n",
    "print(f\"Source Tokens: {src_tokens}\")\n",
    "print(f\"Source Reconstructed: {reconstruct_expression(src_tokens)}\")\n",
    "print(f\"Source String Match: {src_string_match}\")\n",
    "\n",
    "print(\"\\n=== Testing Target Expression ===\")\n",
    "tgt_tokens = tokenizer.tgt_tokenize(tgt_expr)\n",
    "tgt_string_match, tgt_symbolic_match = validate_expression(tgt_expr, tgt_tokens, is_source=False)\n",
    "print(f\"Target Original: {tgt_expr}\")\n",
    "print(f\"Target Tokens: {tgt_tokens}\")\n",
    "print(f\"Target Reconstructed: {reconstruct_expression(tgt_tokens)}\")\n",
    "print(f\"Target String Match: {tgt_string_match}\")\n",
    "print(f\"Target Symbolic Equivalence: {tgt_symbolic_match}\")\n",
    "\n",
    "target_edge_cases = [\n",
    "    \"(1/2)/(3/4)\",\n",
    "    \"10/100\",\n",
    "    \"(m_c^2 + s_12)^2\",\n",
    "    \"1/2\",\n",
    "    \"-1/2\"\n",
    "]\n",
    "source_edge_cases = [\n",
    "    \"gamma_{+\\\\INDEX_0,INDEX_1,INDEX_2}\",\n",
    "    \"b_{MOMENTUM_0,INDEX_4}(p_3)_v\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e453c19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Testing Target Edge Cases ===\n",
      "\n",
      "Edge Case: (1/2)/(3/4)\n",
      "Tokens: ['(', '1', '/', '2', ')', '/', '(', '3', '/', '4', ')']\n",
      "Reconstructed: (1/2)/(3/4)\n",
      "String Match: True\n",
      "Symbolic Equivalence: True\n",
      "\n",
      "Edge Case: 10/100\n",
      "Tokens: ['10', '/', '100']\n",
      "Reconstructed: 10/100\n",
      "String Match: True\n",
      "Symbolic Equivalence: True\n",
      "\n",
      "Edge Case: (m_c^2 + s_12)^2\n",
      "Tokens: ['(', 'm_c', '^', '2', '+', 'MANDELSTAM_12', ')', '^', '2']\n",
      "Reconstructed: (m_c^2+s_12)^2\n",
      "String Match: False\n",
      "Symbolic Equivalence: True\n",
      "\n",
      "Edge Case: 1/2\n",
      "Tokens: ['1', '/', '2']\n",
      "Reconstructed: 1/2\n",
      "String Match: True\n",
      "Symbolic Equivalence: True\n",
      "\n",
      "Edge Case: -1/2\n",
      "Tokens: ['-', '1', '/', '2']\n",
      "Reconstructed: -1/2\n",
      "String Match: True\n",
      "Symbolic Equivalence: True\n",
      "\n",
      "=== Testing Source Edge Cases ===\n",
      "\n",
      "Edge Case: gamma_{+\\INDEX_0,INDEX_1,INDEX_2}\n",
      "Tokens: ['gamma_', '{', '+', '\\\\', 'INDEX_0', ',', 'INDEX_1', ',', 'INDEX_2', '}']\n",
      "Reconstructed: gamma_{+\\INDEX_0,INDEX_1,INDEX_2}\n",
      "String Match: True\n",
      "\n",
      "Edge Case: b_{MOMENTUM_0,INDEX_4}(p_3)_v\n",
      "Tokens: ['b_', '{', 'MOMENTUM_0', ',', 'INDEX_4', '}', '(', 'P_3', ')', '_', 'v']\n",
      "Reconstructed: b_{MOMENTUM_0,INDEX_4}(p_3)_v\n",
      "String Match: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Testing Target Edge Cases ===\")\n",
    "for expr in target_edge_cases:\n",
    "    try:\n",
    "        tokens = tokenizer.tgt_tokenize(expr)\n",
    "        string_match, symbolic_match = validate_expression(expr, tokens, is_source=False)\n",
    "        reconstructed = reconstruct_expression(tokens)\n",
    "        print(f\"\\nEdge Case: {expr}\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Reconstructed: {reconstructed}\")\n",
    "        print(f\"String Match: {string_match}\")\n",
    "        print(f\"Symbolic Equivalence: {symbolic_match}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nEdge Case: {expr}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\n=== Testing Source Edge Cases ===\")\n",
    "for expr in source_edge_cases:\n",
    "    try:\n",
    "        tokens = tokenizer.src_tokenize(expr)\n",
    "        string_match, _ = validate_expression(expr, tokens, is_source=True)\n",
    "        reconstructed = reconstruct_expression(tokens)\n",
    "        print(f\"\\nEdge Case: {expr}\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Reconstructed: {reconstructed}\")\n",
    "        print(f\"String Match: {string_match}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nEdge Case: {expr}\")\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b3acbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building Vocabularies ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing source vocab: 100%|██████████| 9952/9952 [00:06<00:00, 1505.91it/s]\n",
      "Processing target vocab: 100%|██████████| 9952/9952 [00:04<00:00, 2362.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Vocabulary Size: 78 tokens\n",
      "Target Vocabulary Total Size (with special tokens): 45\n",
      "Source Vocabulary Total Size (with special tokens): 83\n",
      "Target Vocabulary Total Size (with special tokens): 50\n",
      "\n",
      "Vocabularies saved to 'src_vocab.txt' and 'tgt_vocab.txt'.\n",
      "Token-to-index mappings saved to 'src_vocab.json' and 'tgt_vocab.json'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Building Vocabularies ===\")\n",
    "src_vocab_set = tokenizer.build_src_vocab()\n",
    "tgt_vocab_set = tokenizer.build_tgt_vocab()\n",
    "print(f\"Source Vocabulary Size: {len(src_vocab_set)} tokens\")\n",
    "print(f\"Target Vocabulary Total Size (with special tokens): {len(tgt_vocab_set)}\")\n",
    "\n",
    "src_vocab = SymbolicVocab(\n",
    "    tokens=src_vocab_set,\n",
    "    special_symbols=special_symbols,\n",
    "    bos_idx=2,\n",
    "    pad_idx=0,\n",
    "    eos_idx=3,\n",
    "    unk_idx=1,\n",
    "    sep_idx=4\n",
    ")\n",
    "tgt_vocab = SymbolicVocab(\n",
    "    tokens=tgt_vocab_set,\n",
    "    special_symbols=special_symbols,\n",
    "    bos_idx=2,\n",
    "    pad_idx=0,\n",
    "    eos_idx=3,\n",
    "    unk_idx=1,\n",
    "    sep_idx=4\n",
    ")\n",
    "\n",
    "print(f\"Source Vocabulary Total Size (with special tokens): {len(src_vocab)}\")\n",
    "print(f\"Target Vocabulary Total Size (with special tokens): {len(tgt_vocab)}\")\n",
    "\n",
    "src_vocab.save('src_vocab.txt')\n",
    "tgt_vocab.save('tgt_vocab.txt')\n",
    "print(\"\\nVocabularies saved to 'src_vocab.txt' and 'tgt_vocab.txt'.\")\n",
    "print(\"Token-to-index mappings saved to 'src_vocab.json' and 'tgt_vocab.json'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2057582b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example Encoding/Decoding ===\n",
      "Sample Target Expression: 1/81*e^4*(16*m_b^2*m_c^2 + 8*m_b^2*s_12 + 8*s_14*s_23 + 8*s_13*s_24 + 8*m_c^2*s_34)*(m_c^2 + s_12 + 1/2*reg_prop)^(-2)\n",
      "Tokens: ['1', '/', '81', '*', 'E_CHARGE', '^', '4', '*', '(', '16', '*', 'm_b', '^', '2', '*', 'm_c', '^', '2', '+', '8', '*', 'm_b', '^', '2', '*', 'MANDELSTAM_12', '+', '8', '*', 'MANDELSTAM_14', '*', 'MANDELSTAM_23', '+', '8', '*', 'MANDELSTAM_13', '*', 'MANDELSTAM_24', '+', '8', '*', 'm_c', '^', '2', '*', 'MANDELSTAM_34', ')', '*', '(', 'm_c', '^', '2', '+', 'MANDELSTAM_12', '+', '1', '/', '2', '*', 'reg_prop', ')', '^', '(', '-', '2', ')']\n",
      "Encoded: [11, 10, 23, 7, 25, 37, 19, 7, 5, 13, 7, 40, 37, 14, 7, 41, 37, 14, 8, 22, 7, 40, 37, 14, 7, 28, 8, 22, 7, 30, 7, 32, 8, 22, 7, 29, 7, 33, 8, 22, 7, 41, 37, 14, 7, 35, 6, 7, 5, 41, 37, 14, 8, 28, 8, 11, 10, 14, 7, 48, 6, 37, 5, 9, 14, 6]\n",
      "Decoded: ['1', '/', '81', '*', 'E_CHARGE', '^', '4', '*', '(', '16', '*', 'm_b', '^', '2', '*', 'm_c', '^', '2', '+', '8', '*', 'm_b', '^', '2', '*', 'MANDELSTAM_12', '+', '8', '*', 'MANDELSTAM_14', '*', 'MANDELSTAM_23', '+', '8', '*', 'MANDELSTAM_13', '*', 'MANDELSTAM_24', '+', '8', '*', 'm_c', '^', '2', '*', 'MANDELSTAM_34', ')', '*', '(', 'm_c', '^', '2', '+', 'MANDELSTAM_12', '+', '1', '/', '2', '*', 'reg_prop', ')', '^', '(', '-', '2', ')']\n",
      "\n",
      "=== Validating Full Dataset ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating dataset: 100%|██████████| 9952/9952 [00:56<00:00, 175.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Symbolic Equivalence True for 9952/9952 target expressions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Example Encoding/Decoding ===\")\n",
    "sample_tgt_expr = df['sqamp'].iloc[0]\n",
    "sample_tgt_tokens = tokenizer.tgt_tokenize(sample_tgt_expr)\n",
    "encoded = tgt_vocab.encode(sample_tgt_tokens)\n",
    "decoded = tgt_vocab.decode(encoded)\n",
    "print(f\"Sample Target Expression: {sample_tgt_expr}\")\n",
    "print(f\"Tokens: {sample_tgt_tokens}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "print(\"\\n=== Validating Full Dataset ===\")\n",
    "count = 0\n",
    "failed_expressions = []\n",
    "for i in tqdm(range(len(df)), desc=\"Validating dataset\"):\n",
    "    src_expr = df['amp'].iloc[i]\n",
    "    tgt_expr = df['sqamp'].iloc[i]\n",
    "    src_tokens = tokenizer.src_tokenize(src_expr)\n",
    "    tgt_tokens = tokenizer.tgt_tokenize(tgt_expr)\n",
    "    src_string_match, _ = validate_expression(src_expr, src_tokens, is_source=True)\n",
    "    tgt_string_match, tgt_symbolic_match = validate_expression(tgt_expr, tgt_tokens, is_source=False)\n",
    "    if tgt_symbolic_match:\n",
    "        count += 1\n",
    "    else:\n",
    "        print((i, tgt_expr, tgt_tokens, reconstruct_expression(tgt_tokens)))\n",
    "        failed_expressions.append((i, src_expr, tgt_expr, tgt_tokens, reconstruct_expression(tgt_tokens)))\n",
    "\n",
    "print(f\"\\nSymbolic Equivalence True for {count}/{len(df)} target expressions.\")\n",
    "if failed_expressions:\n",
    "    with open('failed_expressions.txt', 'w', encoding='utf-8') as f:\n",
    "        for idx, src, tgt, tokens, recon in failed_expressions:\n",
    "            f.write(f\"Expression {idx+1}:\\n\")\n",
    "            f.write(f\"Source: {src}\\n\")\n",
    "            f.write(f\"Target: {tgt}\\n\")\n",
    "            f.write(f\"Target Tokens: {tokens}\\n\")\n",
    "            f.write(f\"Reconstructed: {recon}\\n\\n\")\n",
    "    print(f\"Logged {len(failed_expressions)} failed expressions to 'failed_expressions.txt'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8f5cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
