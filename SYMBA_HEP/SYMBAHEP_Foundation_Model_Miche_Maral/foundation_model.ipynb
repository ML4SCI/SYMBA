{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd2f2a6",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a07b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "from google.colab import drive\n",
    "\n",
    "# --- Google Drive Mounting ---\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# --- Path Configuration ---\n",
    "DRIVE_FOLDER = '/content/drive/My Drive/Data Files No Momentum Permutation May 2025-selected'\n",
    "OUTPUT_FILE_PATH = '/content/drive/My Drive/processed_expressions_output_final12.txt'\n",
    "\n",
    "# Configure error limits for debugging\n",
    "MAX_TOTAL_ERROR_LINES = 1000 \n",
    "\n",
    "# --- Physics Profile Definitions ---\n",
    "def get_physics_profile(theory):\n",
    "    profiles = {\n",
    "        \"QED\": {\n",
    "            \"particles\": [\"e\", \"mu\", \"tau\", \"A\"], \"constants\": [\"e\", \"m_e\", \"m_mu\", \"m_tau\", \"reg_prop\"],\n",
    "            \"operators\": [\"+\", \"-\", \"*\", \"/\", \"**\", \"^\"],\n",
    "            \"functions\": [\"sin\", \"cos\", \"exp\", \"log\", \"sqrt\", \"cbrt\", \"atan2\", \"erf\", \"gamma\", \"digamma\"],\n",
    "            \"special_terms\": [\"P_L\", \"P_R\", \"gamma\"]\n",
    "        },\n",
    "        \"EW\": {\n",
    "            \"particles\": [\"e\", \"mu\", \"tau\", \"nue_L\", \"numu_L\", \"nutau_L\", \"u\", \"d\", \"c\", \"s\", \"t\", \"b\", \"A\", \"Z\", \"W\", \"h\"],\n",
    "            \"constants\": [\"e\", \"v\", \"theta_W\", \"m_W\", \"m_Z\", \"m_h\", \"m_u\", \"m_d\", \"m_c\", \"m_s\", \"m_t\", \"m_b\", \"reg_prop\", \"gs\"],\n",
    "            \"operators\": [\"+\", \"-\", \"*\", \"/\", \"**\", \"^\"],\n",
    "            \"functions\": [\"sin\", \"cos\", \"exp\", \"log\", \"sqrt\", \"cbrt\", \"atan2\", \"erf\", \"gamma\", \"digamma\"],\n",
    "            \"special_terms\": [\"P_L\", \"P_R\", \"gamma\"]\n",
    "        },\n",
    "        \"QCD\": {\n",
    "            \"particles\": [\"u\", \"d\", \"c\", \"s\", \"t\", \"b\", \"G\"],\n",
    "            \"constants\": [\"gs\", \"g\", \"m_u\", \"m_d\", \"m_c\", \"m_s\", \"m_t\", \"m_b\", \"reg_prop\"],\n",
    "            \"operators\": [\"+\", \"-\", \"*\", \"/\", \"**\", \"^\"],\n",
    "            \"functions\": [\"sin\", \"cos\", \"exp\", \"log\", \"sqrt\", \"cbrt\", \"atan2\", \"erf\", \"gamma\", \"digamma\"],\n",
    "            \"special_terms\": [\"gamma\", \"T_C\"]\n",
    "        }\n",
    "    }\n",
    "    selected_profile = profiles.get(theory)\n",
    "    if selected_profile:\n",
    "        selected_profile['name'] = theory\n",
    "    return selected_profile\n",
    "\n",
    "\n",
    "# --- Core Processing Functions ---\n",
    "\n",
    "def clean_physics_expression(line: str, filename: str, line_num: int) -> tuple[str, list]:\n",
    "    \"\"\"\n",
    "    Cleans a single line containing a physics expression, transforming problematic\n",
    "    LaTeX-like sequences into a program-friendly, flat string suitable for symbolic regression.\n",
    "    Retains physics knowledge by consistently naming variables.\n",
    "    Returns the cleaned string and a list of any unrecognized snippets found during cleanup.\n",
    "    \"\"\"\n",
    "    raw_expression = line.strip()\n",
    "    unrecognized_snippets_in_line = []\n",
    "\n",
    "    expression_start_marker = \"Interaction: \"\n",
    "    if expression_start_marker not in raw_expression:\n",
    "        return raw_expression, []\n",
    "\n",
    "    # --- Step 1: Robustly isolate the core mathematical formula and description ---\n",
    "\n",
    "    # 1. Start by stripping the \"Interaction: \" prefix\n",
    "    temp_line = raw_expression.replace(expression_start_marker, '', 1).strip()\n",
    "\n",
    "    # 2. **Crucial:** Remove the final numerical result from the end of the line.\n",
    "    # This pattern looks for \" : \" followed by a number/fraction, potentially with 'e' or 'i',\n",
    "    # and then possibly other characters until the end of the line.\n",
    "    # It identifies the *last* major numerical block that's typically the final output value.\n",
    "    final_numerical_result_pattern = r'\\s*:\\s*([-+]?(?:(?:\\d+\\.?\\d*(?:[eE][+-]?\\d+)?(?:/\\d+\\.?\\d*)?)|(?:[iI])(?:\\*(?:\\d+\\.?\\d*(?:[eE][+-]?\\d+)?(?:/\\d+\\.?\\d*)?))?)\\s*(?:[^\\s\\w]|\\s[^\\s\\w]|\\w|\\.)*)$'\n",
    "\n",
    "    match_final_result = re.search(final_numerical_result_pattern, temp_line)\n",
    "    if match_final_result:\n",
    "        temp_line = temp_line[:match_final_result.start()].strip()\n",
    "\n",
    "    # 3. Remove \" : Error evaluating combination:...\" if present.\n",
    "    # This often appears right before the final numerical result, or if the calculation failed.\n",
    "    error_marker = \" : Error evaluating combination:\"\n",
    "    error_idx = temp_line.rfind(error_marker)\n",
    "    if error_idx != -1:\n",
    "        temp_line = temp_line[:error_idx].strip()\n",
    "\n",
    "    # At this point, temp_line should ideally contain:\n",
    "    # `[Initial Description] : [Vertex/OffShell/Particle Prose] : [Mathematical Formula]`\n",
    "    # The last colon (:) in this remaining `temp_line` should precede the actual formula.\n",
    "\n",
    "    description_part = \"\"\n",
    "    formula_part = \"\"\n",
    "\n",
    "    # Find the LAST colon that separates the prose from the complex formula.\n",
    "    # We look for a colon that is NOT followed by 'Vertex' or 'OffShell' (which indicates prose)\n",
    "    # This specifically aims to find the colon just before the start of the mathematical formula.\n",
    "    # The mathematical formula starts with a number, a sign, 'i', or a parenthesis.\n",
    "\n",
    "    # Regex to find the colon *before* the actual mathematical part\n",
    "    # This is a very precise pattern for the *start* of the actual algebraic expression.\n",
    "    # It looks for ': ' followed by optional signs, numbers/fractions, 'i', 'e', or an open parenthesis.\n",
    "    formula_start_marker_in_middle = r'\\s*:\\s*([-+]?\\s*(?:\\d+\\s*|i\\s*|e\\s*|\\()\\s*(?:/|\\*\\*|\\*|\\+|\\-|\\().*)'\n",
    "\n",
    "    formula_match_in_middle = re.search(formula_start_marker_in_middle, temp_line)\n",
    "\n",
    "    if formula_match_in_middle:\n",
    "        # The description is everything before this colon and its preceding space.\n",
    "        description_part = temp_line[:formula_match_in_middle.start()].strip()\n",
    "        # The formula is everything from the first capture group (the actual math content).\n",
    "        formula_part = formula_match_in_middle.group(1).strip()\n",
    "    else:\n",
    "        # Fallback: if no clear formula start found, assume the whole remaining line is description.\n",
    "        description_part = temp_line.strip()\n",
    "        formula_part = \"\"\n",
    "\n",
    "    # Clean the `description_part` of all remaining prose and colons.\n",
    "    # This part should be free of math components and only contain simplified particle info.\n",
    "    description_part = re.sub(r'Vertex V_\\d+:[^,]+,?\\s*', '', description_part)\n",
    "    description_part = re.sub(r'OffShell\\s+\\w+(?:\\[\\w+\\])?(?:,\\s*)?', '', description_part)\n",
    "    description_part = re.sub(r'AntiPart\\s+\\w+(?:\\[\\w+\\])?(?:,\\s*)?', '', description_part)\n",
    "    description_part = re.sub(r'\\s*to\\s*', ' to ', description_part)\n",
    "    description_part = re.sub(r'\\([A-Za-z]_\\d+\\)', '', description_part)\n",
    "    description_part = description_part.replace('(X)', '')\n",
    "    description_part = description_part.replace(':', ' ').strip() # Remove any remaining colons\n",
    "    description_part = re.sub(r'\\s+', ' ', description_part).strip() # Collapse multiple spaces\n",
    "\n",
    "\n",
    "    # Combine the cleaned description and the extracted formula with a single, clean colon\n",
    "    if formula_part:\n",
    "        cleaned_line = f\"{description_part} : {formula_part}\".strip()\n",
    "    else:\n",
    "        # If no formula was extracted, the line just contains the cleaned description.\n",
    "        cleaned_line = description_part.strip()\n",
    "\n",
    "\n",
    "    # --- Step 2: Apply specific cleaning rules to flatten LaTeX-like symbols within the remaining expression ---\n",
    "    # These rules apply to the entire `cleaned_line`, but are mostly relevant for `formula_part`.\n",
    "\n",
    "    # Convert power operator\n",
    "    cleaned_line = cleaned_line.replace('^(*)', ' conj ')\n",
    "    cleaned_line = cleaned_line.replace('^', ' ** ')\n",
    "\n",
    "    # Regex for gamma functions and indices:\n",
    "    def process_gamma_indices(match):\n",
    "        content = match.group(1)\n",
    "        is_plus = False\n",
    "        if content.startswith('\\\\+') or content.startswith('+'):\n",
    "            is_plus = True\n",
    "            content = content[2:] if content.startswith('\\\\+') else content[1:]\n",
    "\n",
    "        content = re.sub(r'[\\\\%]([a-zA-Z]+)_(\\d+)', r'\\1\\2', content)\n",
    "        content = content.replace(',', '_')\n",
    "        content = content.replace('__', '_').strip('_')\n",
    "\n",
    "        if is_plus:\n",
    "            return f\"gamma_plus_{content}\"\n",
    "        else:\n",
    "            return f\"gamma_{content}\"\n",
    "\n",
    "    cleaned_line = re.sub(r'gamma_\\{([^{}]+)\\}', process_gamma_indices, cleaned_line)\n",
    "\n",
    "\n",
    "    # Handle generic variable with complex subscript like A_{i_3,+sigma_166}\n",
    "    def process_general_subscript_indices(match):\n",
    "        prefix = match.group(1)\n",
    "        content = match.group(2)\n",
    "\n",
    "        content = re.sub(r'[\\\\%]([a-zA-Z]+)_(\\d+)', r'\\1\\2', content)\n",
    "        content = content.replace(',', '_').replace('+', '_plus_')\n",
    "        content = content.replace('__', '_').strip('_')\n",
    "\n",
    "        return f\"{prefix}_{content}\"\n",
    "\n",
    "    cleaned_line = re.sub(r'([A-Za-z])_\\{([^{}]+)\\}', process_general_subscript_indices, cleaned_line)\n",
    "\n",
    "\n",
    "    # Handle P_L_{del_633,eps_289} -> P_L_del633_eps289\n",
    "    cleaned_line = re.sub(r'P_([LR])_\\{([a-zA-Z]+)_(\\d+),([a-zA-Z]+)_(\\d+)\\}',\n",
    "                          r'P_\\1_\\2\\3_\\4\\5', cleaned_line)\n",
    "\n",
    "    # Handle p_X_+sigma_NUMBER: p_5_+sigma_1326 -> p_5_plus_sigma1326\n",
    "    cleaned_line = re.sub(r'p_(\\d+)_?\\+([a-zA-Z]+)_(\\d+)', r'p_\\1_plus_\\2\\3', cleaned_line)\n",
    "\n",
    "    # Handle p_X_greek_NUMBER: p_1_rho_304 -> p_1_rho304\n",
    "    cleaned_line = re.sub(r'p_(\\d+)_([a-zA-Z]+)_(\\d+)', r'p_\\1_\\2\\3', cleaned_line)\n",
    "\n",
    "    # Handle a_i_X pattern (from A_i_3 in earlier output) -> a_i3\n",
    "    cleaned_line = re.sub(r'([a-zA-Z])_([a-zA-Z])_(\\d+)', r'\\1_\\2\\3', cleaned_line)\n",
    "\n",
    "    # Handle (p_#)_u/v patterns (e.g., (p_1)_u -> p1_u)\n",
    "    cleaned_line = re.sub(r'\\(p_(\\d+)\\)_([uv])', r'p\\1_\\2', cleaned_line)\n",
    "    # Also handle the conj suffix correctly: gam_248_u conj -> gam_248_u_conj\n",
    "    cleaned_line = re.sub(r'([a-zA-Z]+_\\d+)_([uv])\\s*(conj)', r'\\1_\\2_\\3', cleaned_line)\n",
    "    # Generic conj without u/v\n",
    "    cleaned_line = re.sub(r'([a-zA-Z]+_\\d+)\\s*(conj)', r'\\1_conj', cleaned_line)\n",
    "\n",
    "    # Remove any remaining solitary backslashes or percent signs, and curly braces\n",
    "    cleaned_line = cleaned_line.replace('\\\\', '')\n",
    "    cleaned_line = cleaned_line.replace('%', '')\n",
    "    cleaned_line = cleaned_line.replace('{', '')\n",
    "    cleaned_line = cleaned_line.replace('}', '')\n",
    "\n",
    "\n",
    "    # Standardize spacing around operators and common delimiters\n",
    "    for op in ['**', '*', '/', '+', '-', '=', '(', ')', '[', ']']:\n",
    "        cleaned_line = cleaned_line.replace(op, f' {op} ')\n",
    "\n",
    "    # Replace commas that are *not* part of numbers (e.g., in `1/2`) but are separators.\n",
    "    # At this stage, most commas should be converted if part of a name.\n",
    "    # Remaining ones are probably leftovers from lists/prose, so convert to space.\n",
    "    cleaned_line = cleaned_line.replace(',', ' ')\n",
    "    cleaned_line = re.sub(r'\\s+', ' ', cleaned_line).strip() # Collapse multiple spaces\n",
    "\n",
    "\n",
    "    # Step 3: Identify any remaining problematic characters (after all cleaning attempts)\n",
    "    # This pattern should now only catch characters that are truly unexpected and not part of\n",
    "    # valid variable names, numbers, or standard mathematical operations/delimiters.\n",
    "    problematic_character_pattern = r'[^a-zA-Z0-9_.\\s+\\-*/()\\[\\]=:]' # Re-added colon to catch if it *still* appears unexpectedly\n",
    "\n",
    "    current_index = 0\n",
    "    temp_cleaned_line = cleaned_line\n",
    "    while current_index < len(temp_cleaned_line):\n",
    "        match = re.search(problematic_character_pattern, temp_cleaned_line[current_index:])\n",
    "\n",
    "        if match:\n",
    "            start_of_match = current_index + match.start()\n",
    "            end_of_match = current_index + match.end()\n",
    "            snippet = temp_cleaned_line[start_of_match:end_of_match]\n",
    "            if snippet.strip() and snippet not in [' ', '\\t']:\n",
    "                unrecognized_snippets_in_line.append({\n",
    "                    'snippet': snippet,\n",
    "                    'index': start_of_match,\n",
    "                    'type': 'uncleaned_char'\n",
    "                })\n",
    "            current_index = end_of_match\n",
    "            if len(unrecognized_snippets_in_line) >= 5:\n",
    "                unrecognized_snippets_in_line.append({'snippet': '...', 'index': -1, 'type': 'truncated'})\n",
    "                break\n",
    "        else:\n",
    "            current_index = len(temp_cleaned_line)\n",
    "\n",
    "    return cleaned_line, unrecognized_snippets_in_line\n",
    "\n",
    "\n",
    "# --- Main Execution Loop (remains unchanged) ---\n",
    "if __name__ == \"__main__\":\n",
    "    all_processed_data = []\n",
    "    all_unrecognized_snippets_summary = {}\n",
    "    total_error_lines_count = 0\n",
    "\n",
    "    print(f\"Attempting to list files in: {DRIVE_FOLDER}\")\n",
    "\n",
    "    try:\n",
    "        os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
    "        expression_files = [\n",
    "            os.path.join(DRIVE_FOLDER, f)\n",
    "            for f in os.listdir(DRIVE_FOLDER)\n",
    "            if f.endswith('.txt')\n",
    "        ]\n",
    "        print(f\"Found {len(expression_files)} .txt files.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The folder '{DRIVE_FOLDER}' was not found. Please ensure it exists on your Google Drive.\")\n",
    "        expression_files = []\n",
    "\n",
    "    with open(OUTPUT_FILE_PATH, 'w', encoding='utf-8') as outfile:\n",
    "        print(f\"Output logs and cleaned expressions will be saved to: {OUTPUT_FILE_PATH}\\n\", file=outfile)\n",
    "        print(f\"Starting processing of files from: {DRIVE_FOLDER}\\n\")\n",
    "\n",
    "        break_file_loop = False\n",
    "        for filepath in expression_files:\n",
    "            if break_file_loop:\n",
    "                break\n",
    "\n",
    "            print(f\"\\nProcessing file: {os.path.basename(filepath)}\")\n",
    "            print(f\"\\n# --- Processing file: {filepath} --- #\", file=outfile)\n",
    "\n",
    "            lines_in_current_file = 0\n",
    "            errors_in_current_file = 0\n",
    "\n",
    "            try:\n",
    "                filename = os.path.basename(filepath)\n",
    "                physics_theory = \"Unknown\"\n",
    "                if \"QED\" in filename.upper(): physics_theory = \"QED\"\n",
    "                elif \"EW\" in filename.upper(): physics_theory = \"EW\"\n",
    "                elif \"QCD\" in filename.upper(): physics_theory = \"QCD\"\n",
    "\n",
    "                process_type = \"unknown\"\n",
    "                calculation_level = \"unknown\"\n",
    "                diagram_id = \"n/a\"\n",
    "\n",
    "                filename_base = os.path.splitext(filename)[0]\n",
    "                filename_parts = filename_base.lower().split('-')\n",
    "\n",
    "                try:\n",
    "                    level_index = filename_parts.index('treelevel')\n",
    "                    calculation_level = 'tree'\n",
    "                    if level_index + 1 < len(filename_parts) and filename_parts[level_index + 1].isdigit():\n",
    "                        diagram_id = filename_parts[level_index + 1]\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    to_index = filename_parts.index('to')\n",
    "                    if to_index > 0 and to_index < len(filename_parts) - 1:\n",
    "                        process_type = f\"{filename_parts[to_index-1]}-to-{filename_parts[to_index+1]}\"\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "                metadata_log = f\"    Identified Theory: {physics_theory}, Process: {process_type}, Level: {calculation_level}, ID: {diagram_id}\"\n",
    "                print(metadata_log)\n",
    "                print(metadata_log, file=outfile)\n",
    "\n",
    "                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    for line_num, line in enumerate(f):\n",
    "                        lines_in_current_file += 1\n",
    "                        if \"Interaction:\" in line:\n",
    "                            cleaned_expression, unrecognized_snippets = clean_physics_expression(line, filename, line_num + 1)\n",
    "\n",
    "                            if unrecognized_snippets:\n",
    "                                total_error_lines_count += 1\n",
    "                                errors_in_current_file += 1\n",
    "                                first_problematic_snippet_info = unrecognized_snippets[0]\n",
    "                                error_msg = f\"    Cleaning issues on line {line_num + 1} in {filename}. First problematic snippet: '{first_problematic_snippet_info['snippet']}'.\"\n",
    "                                print(error_msg)\n",
    "                                print(error_msg, file=outfile)\n",
    "                                for snippet_info in unrecognized_snippets:\n",
    "                                    snippet_key = snippet_info['snippet'].split(' ')[0]\n",
    "                                    all_unrecognized_snippets_summary[snippet_key] = all_unrecognized_snippets_summary.get(snippet_key, 0) + 1\n",
    "\n",
    "                                if total_error_lines_count >= MAX_TOTAL_ERROR_LINES:\n",
    "                                    print(f\"\\nStopping processing early due to exceeding {MAX_TOTAL_ERROR_LINES} lines with cleaning errors.\")\n",
    "                                    break_file_loop = True\n",
    "                                    break\n",
    "                            else:\n",
    "                                all_processed_data.append({\n",
    "                                    'theory': physics_theory,\n",
    "                                    'process_type': process_type,\n",
    "                                    'calculation_level': calculation_level,\n",
    "                                    'diagram_id': diagram_id,\n",
    "                                    'filename': filename,\n",
    "                                    'line_number': line_num + 1,\n",
    "                                    'original_line': line.strip(),\n",
    "                                    'cleaned_expression': cleaned_expression,\n",
    "                                })\n",
    "                                print(f\"    Cleaned line {line_num + 1}: {cleaned_expression}\", file=outfile)\n",
    "                        else:\n",
    "                            outfile.write(line.strip() + '\\n')\n",
    "\n",
    "                        if break_file_loop:\n",
    "                            break\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Failed to read or process file {filepath}: {e}\"\n",
    "                print(error_msg)\n",
    "                print(error_msg, file=outfile)\n",
    "\n",
    "            file_successful_lines_count = lines_in_current_file - errors_in_current_file\n",
    "            file_success_percentage = (file_successful_lines_count / lines_in_current_file) * 100 if lines_in_current_file > 0 else 0\n",
    "            print(f\"    File Summary: {file_successful_lines_count}/{lines_in_current_file} lines processed successfully ({file_success_percentage:.2f}%).\")\n",
    "            print(f\"    File Summary: {file_successful_lines_count}/{lines_in_current_file} lines processed successfully ({file_success_percentage:.2f}%).\", file=outfile)\n",
    "\n",
    "        summary_header = \"\\n\\n--- Processing Summary ---\"\n",
    "        print(summary_header)\n",
    "        print(summary_header, file=outfile)\n",
    "\n",
    "        total_interaction_lines_in_files = 0\n",
    "        for filepath in expression_files:\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    for line in f:\n",
    "                        if \"Interaction:\" in line:\n",
    "                            total_interaction_lines_in_files += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        total_lines_attempted_cleaning = total_interaction_lines_in_files\n",
    "        summary_successful_lines = len(all_processed_data)\n",
    "        summary_error_lines = total_error_lines_count\n",
    "\n",
    "        global_success_percentage = 0\n",
    "        if total_lines_attempted_cleaning > 0:\n",
    "            global_success_percentage = (summary_successful_lines / total_lines_attempted_cleaning) * 100\n",
    "\n",
    "        print(f\"Total interaction lines attempted to clean: {total_lines_attempted_cleaning}\")\n",
    "        print(f\"Total successfully cleaned interaction lines: {summary_successful_lines}\")\n",
    "        print(f\"Total interaction lines with cleaning errors: {summary_error_lines}\")\n",
    "        print(f\"Overall Interaction Cleaning Success Rate: {global_success_percentage:.2f}%\")\n",
    "        print(f\"Total interaction lines attempted to clean: {total_lines_attempted_cleaning}\", file=outfile)\n",
    "        print(f\"Total successfully cleaned interaction lines: {summary_successful_lines}\", file=outfile)\n",
    "        print(f\"Total interaction lines with cleaning errors: {summary_error_lines}\", file=outfile)\n",
    "        print(f\"Overall Interaction Cleaning Success Rate: {global_success_percentage:.2f}%\", file=outfile)\n",
    "\n",
    "\n",
    "        if all_unrecognized_snippets_summary:\n",
    "            print(\"\\n--- Summary of Unrecognized Snippets (Top 10) ---\")\n",
    "            sorted_snippets = sorted(all_unrecognized_snippets_summary.items(), key=lambda item: item[1], reverse=True)\n",
    "            for snippet, count in sorted_snippets[:10]:\n",
    "                print(f\"Snippet: '{snippet}' - Occurrences: {count}\")\n",
    "                print(f\"Snippet: '{snippet}' - Occurrences: {count}\", file=outfile)\n",
    "\n",
    "            print(\"\\nTo improve cleaning, add new `re.sub` rules in `clean_physics_expression` for these snippets.\")\n",
    "            print(\"\\nTo improve cleaning, add new `re.sub` rules in `clean_physics_expression` for these snippets.\", file=outfile)\n",
    "\n",
    "\n",
    "        if all_processed_data:\n",
    "            print(\"\\nFirst 3 successfully cleaned interactions (preview):\")\n",
    "            for i, entry in enumerate(all_processed_data[:3]):\n",
    "                print(\"-\" * 30)\n",
    "                print(f\"Interaction {i+1}:\")\n",
    "                print(f\"    File: {entry['filename']} (Line: {entry['line_number']})\")\n",
    "                print(f\"    Details: {entry['theory']}, Process={entry['process_type']}, Level={entry['calculation_level']}, ID={entry['diagram_id']}\")\n",
    "                print(f\"    Original: {entry['original_line'][:120]}...\")\n",
    "                print(f\"    Cleaned: {entry['cleaned_expression'][:120]}...\")\n",
    "                print(\"-\" * 30)\n",
    "\n",
    "    print(f\"\\nFinished. All processing logs and cleaned expressions have been saved to {OUTPUT_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994393c2",
   "metadata": {},
   "source": [
    "Token Embedding Layer & the Mamba Blocks for Symbolic Expression Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060eae3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "\n",
    "# --- MambaBlock definition with causal padding and cropping ---\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.expand = expand\n",
    "\n",
    "        self.in_proj = nn.Linear(d_model, expand * d_model * 2)\n",
    "\n",
    "        # Causal padding: padding = kernel_size - 1\n",
    "        # The conv_layer will output a sequence of length L_in + (kernel_size - 1)\n",
    "        self.conv_layer = nn.Conv1d(\n",
    "            expand * d_model, expand * d_model,\n",
    "            kernel_size=d_conv,\n",
    "            groups=expand * d_model,\n",
    "            padding=d_conv - 1, # Padding for causal conv\n",
    "            bias=False # Often no bias in causal conv for simplicity\n",
    "        )\n",
    "        self.silu = nn.SiLU()\n",
    "        self.out_proj = nn.Linear(expand * d_model, d_model)\n",
    "\n",
    "        self.A = nn.Parameter(torch.randn(expand * d_model, d_state))\n",
    "        self.B = nn.Linear(expand * d_model, d_state)\n",
    "        self.C = nn.Linear(expand * d_model, d_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, d_model)\n",
    "        seq_len = x.shape[1] # Should be 555\n",
    "\n",
    "        xz = self.in_proj(x)\n",
    "        x_proj, z = xz.chunk(2, dim=-1)\n",
    "\n",
    "        # Permute for Conv1d: (B, C, L)\n",
    "        x_conv_permuted = x_proj.permute(0, 2, 1) # (B, C, L)\n",
    "\n",
    "        # Apply convolution with causal padding.\n",
    "        # This will result in an output length of L_in + (d_conv - 1)\n",
    "        x_conv_output = self.conv_layer(x_conv_permuted)\n",
    "\n",
    "        # Crop the output to match the original sequence length\n",
    "        # Remove the extra padding on the right side if padding was applied to both sides\n",
    "        # For a true causal conv, padding is only on the left.\n",
    "        # Conv1d's `padding` parameter adds symmetrically.\n",
    "        # The correct way to implement causal padding is often `padding_mode='zeros'` with `padding=(kernel_size - 1, 0)` if 2D.\n",
    "        # For Conv1d, padding adds to both sides.\n",
    "        # After `padding=d_conv-1`, the length is `seq_len + d_conv - 1`.\n",
    "        # Slice from the beginning up to `seq_len`.\n",
    "\n",
    "        x_conv = x_conv_output[..., :seq_len] # To match original seq_len\n",
    "        x_conv = x_conv.permute(0, 2, 1) # Permute back to (B, L, C)\n",
    "        x_conv = self.silu(x_conv)\n",
    "\n",
    "        ssm_output = self.silu(x_conv)\n",
    "        out = ssm_output * self.silu(z) # Element-wise multiplication, now sizes should match\n",
    "\n",
    "        out = self.out_proj(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SymbolicExpressionEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_mamba_layers, max_seq_len, padding_token_id):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model, padding_idx=padding_token_id)\n",
    "        self.positional_encoding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.mamba_layers = nn.ModuleList([MambaBlock(d_model) for _ in range(num_mamba_layers)])\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.token_embedding(input_ids)\n",
    "        positions = torch.arange(input_ids.size(1), device=input_ids.device).unsqueeze(0)\n",
    "        positional_embeddings = self.positional_encoding(positions)\n",
    "        embeddings = embeddings + positional_embeddings\n",
    "\n",
    "        x = embeddings\n",
    "        for layer in self.mamba_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        symbolic_expression_embedding = x.mean(dim=1) # Mean pooling for the final embedding\n",
    "\n",
    "        return x, symbolic_expression_embedding\n",
    "\n",
    "# --- Custom Dataset and DataLoader ---\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, input_ids, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.input_ids[idx], 'labels': self.labels[idx]}\n",
    "\n",
    "# --- Load Preprocessing Outputs ---\n",
    "# CHANGED PATH HERE:\n",
    "output_dir = \"/content/processed_expressions_output_final12 (3).txt\"\n",
    "# Vocabulary and dataset summary are part of this single file or need to be parsed from it\n",
    "# This assumption might need adjustment based on how processed_expressions_output_final12 (3).txt is structured\n",
    "vocabulary_file = output_dir # If vocabulary is embedded, parse it\n",
    "dataset_summary_file = output_dir # If summary is embedded, parse it\n",
    "\n",
    "vocab_size = 71\n",
    "padding_token_id = 0\n",
    "unknown_token_id = 1\n",
    "number_token_id = 4\n",
    "math_token_id = 5\n",
    "max_seq_len = 555 \n",
    "\n",
    "# --- DataLoaders using preprocessed data ---\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Padding Token ID: {padding_token_id}\")\n",
    "print(f\"Unknown Token ID: {unknown_token_id}\")\n",
    "print(f\"Number Token ID: {number_token_id}\")\n",
    "print(f\"Math Token ID: {math_token_id}\")\n",
    "print(f\"Max Sequence Length (from preprocessing): {max_seq_len}\")\n",
    "\n",
    "train_dataset_size = 88468\n",
    "val_dataset_size = 29490\n",
    "test_dataset_size = 29490\n",
    "batch_size = 2\n",
    "\n",
    "dummy_train_input_ids = torch.randint(0, vocab_size, (train_dataset_size, max_seq_len))\n",
    "dummy_train_labels = torch.randint(0, 2, (train_dataset_size,))\n",
    "\n",
    "dummy_val_input_ids = torch.randint(0, vocab_size, (val_dataset_size, max_seq_len))\n",
    "dummy_val_labels = torch.randint(0, 2, (val_dataset_size,))\n",
    "\n",
    "dummy_test_input_ids = torch.randint(0, vocab_size, (test_dataset_size, max_seq_len))\n",
    "dummy_test_labels = torch.randint(0, 2, (test_dataset_size,))\n",
    "\n",
    "\n",
    "# Create PyTorch Dataset objects\n",
    "train_dataset = TextClassificationDataset(dummy_train_input_ids, dummy_train_labels)\n",
    "val_dataset = TextClassificationDataset(dummy_val_input_ids, dummy_val_labels)\n",
    "test_dataset = TextClassificationDataset(dummy_test_input_ids, dummy_test_labels)\n",
    "\n",
    "# Create PyTorch DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nTrain dataset size: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)} samples\")\n",
    "print(f\"Test dataset size: {len(test_dataset)} samples\")\n",
    "print(f\"Train DataLoader created with batch size: {train_dataloader.batch_size}\")\n",
    "print(f\"Validation DataLoader created with batch size: {val_dataloader.batch_size}\")\n",
    "print(f\"Test DataLoader created with batch size: {test_dataloader.batch_size}\")\n",
    "\n",
    "\n",
    "# --- Initialize the Symbolic Expression Encoder (Mamba) ---\n",
    "d_model = 768 # Dimensionality of the model's embeddings\n",
    "num_mamba_layers = 6 # Number of Mamba blocks in the encoder\n",
    "\n",
    "symbolic_encoder = SymbolicExpressionEncoder(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_mamba_layers=num_mamba_layers,\n",
    "    max_seq_len=max_seq_len,\n",
    "    padding_token_id=padding_token_id\n",
    ")\n",
    "\n",
    "print(\"\\n--- Symbolic Expression Encoder (Mamba) Architecture ---\")\n",
    "print(symbolic_encoder)\n",
    "print(f\"Total parameters in Symbolic Encoder: {sum(p.numel() for p in symbolic_encoder.parameters() if p.requires_grad)}\")\n",
    "\n",
    "# --- Test the encoder with a batch from DataLoader ---\n",
    "print(\"\\n--- Demonstrating Symbolic Expression Encoder with a sample batch from Train DataLoader ---\")\n",
    "# Get one batch from the DataLoader\n",
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    if batch_idx == 0: # Process only the first batch\n",
    "        input_ids = batch['input_ids']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        print(f\"Input IDs shape from DataLoader: {input_ids.shape}\")\n",
    "\n",
    "        # Move model and data to GPU if available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        symbolic_encoder.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "\n",
    "        mamba_hidden_states, symbolic_expression_embedding = symbolic_encoder(input_ids)\n",
    "        print(f\"Mamba Hidden States shape (on {device}): {mamba_hidden_states.shape} (Batch, Sequence Length, d_model)\")\n",
    "        print(f\"Symbolic Expression Embedding shape (on {device}): {symbolic_expression_embedding.shape} (Batch, d_model)\")\n",
    "\n",
    "        break # Exit after the first batch"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
