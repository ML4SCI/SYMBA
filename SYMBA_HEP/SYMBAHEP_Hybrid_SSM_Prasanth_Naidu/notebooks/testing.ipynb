{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42fe77c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/u1/p/pr4santh/Projects/SYMBA_SSM\n"
     ]
    }
   ],
   "source": [
    "cd /global/homes/p/pr4santh/Projects/SYMBA_SSM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da7796eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/global/u1/p/pr4santh/Projects/SYMBA_SSM/model/mamba_encdec.py\", line 19, in <module>\n",
      "    from .helpers.flash_cross_attention import FlashCrossAttentionWrapper\n",
      "ImportError: attempted relative import with no known parent package\n"
     ]
    }
   ],
   "source": [
    "!python mamba_encdec.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd8056c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/p/pr4santh/miniconda3/envs/ssm_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.mamba_encdec import MambaEncDec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c1d59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 8.15kB [00:00, 21.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "mamba_config = {\n",
    "    \"enc_n_layer\": 4,\n",
    "    \"d_model\": 512,\n",
    "    \"dec_n_layer\": 6,\n",
    "    \"rms_norm\": True,\n",
    "    \"fused_add_norm\": True,\n",
    "    \"use_fast_path\": False,\n",
    "    # \"learning_rate\": config.learning_rate,\n",
    "    # \"warmup_steps\": config.warmup_steps,\n",
    "    # \"weight_decay\": config.weight_decay,\n",
    "    # \"devices\": config.devices\n",
    "}\n",
    "#     \"enc_n_layer\": 4,\n",
    "    #     # mamba config\n",
    "    #     \"d_model\": 512,\n",
    "    #     \"n_layer\": 6,\n",
    "    #     \"rms_norm\": True,\n",
    "    #     \"fused_add_norm\": True,\n",
    "    #     \"use_fast_path\": False,\n",
    "    #     \"learning_rate\": 7e-4,\n",
    "    #     \"warmup_steps\": 4000,\n",
    "    #     \"weight_decay\": 0.001,\n",
    "    #     \"devices\": 'cuda:0'\n",
    "model = MambaEncDec(\n",
    "    **mamba_config,\n",
    "    config = mamba_config,\n",
    "    src_vocab_size=300,\n",
    "    tgt_vocab_size=51\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da9ec651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixerModel(\n",
       "  (embedding): Embedding(300, 512)\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x Block(\n",
       "      (mixer): Mamba(\n",
       "        (in_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "        (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)\n",
       "        (act): SiLU()\n",
       "        (x_proj): Linear(in_features=1024, out_features=64, bias=False)\n",
       "        (dt_proj): Linear(in_features=32, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      )\n",
       "      (norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm_f): RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "318a9fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f46fd01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaEncDec(\n",
       "  (encoder): MixerModel(\n",
       "    (embedding): Embedding(300, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (mixer): Mamba(\n",
       "          (in_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "          (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)\n",
       "          (act): SiLU()\n",
       "          (x_proj): Linear(in_features=1024, out_features=64, bias=False)\n",
       "          (dt_proj): Linear(in_features=32, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm_f): RMSNorm()\n",
       "  )\n",
       "  (decoder): MambaDecoder(\n",
       "    (backbone): MixerModel(\n",
       "      (embedding): Embedding(51, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0): Block(\n",
       "          (mixer): Mamba(\n",
       "            (in_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=1024, out_features=64, bias=False)\n",
       "            (dt_proj): Linear(in_features=32, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "        (1): FlashCrossAttentionWrapper(\n",
       "          (attention): Attention(\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "            (attend): Attend(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (rel_pos): RelativePositionBias(\n",
       "            (relative_attention_bias): Embedding(32, 8)\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "        (2): FeedForwardWrapper(\n",
       "          (norm): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (mixer): Mamba(\n",
       "            (in_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
       "            (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=1024, out_features=64, bias=False)\n",
       "            (dt_proj): Linear(in_features=32, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "        (4): FlashCrossAttentionWrapper(\n",
       "          (attention): Attention(\n",
       "            (to_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "            (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "            (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "            (attend): Attend(\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (to_out): Linear(in_features=512, out_features=512, bias=False)\n",
       "          )\n",
       "          (rel_pos): RelativePositionBias(\n",
       "            (relative_attention_bias): Embedding(32, 8)\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "        (5): FeedForwardWrapper(\n",
       "          (norm): RMSNorm()\n",
       "          (mlp): GatedMlp(\n",
       "            (fc1): Linear(in_features=512, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_f): RMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=51, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1feafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bd52a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d4d48d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
